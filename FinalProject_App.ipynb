{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.9 64-bit ('base': conda)",
   "display_name": "Python 3.7.9 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "73a666d948711064f59d854e9125f239b700b6d583285c0d9848266fb8325020"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date,timedelta\n",
    "import json\n",
    "import csv\n",
    "import tweepy\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "from urllib.request import urlopen, Request\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "import yfinance as yf"
   ]
  },
  {
   "source": [
    "## 1. Retrieve All Data:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stock_data(ticker):\n",
    "    end_date=date.today()+timedelta(days=1)\n",
    "    start_date=date.today()\n",
    "    # 1. Request data:\n",
    "    data = yf.download(ticker, \n",
    "                      start=start_date, \n",
    "                      end=end_date,\n",
    "                      interval='30m', \n",
    "                      progress=False)\n",
    "    # 2. Feature Engineering:\n",
    "    data['Percent Price Change Within Period'] = ((data['Close'] - data['Open'])/data['Open'])*100\n",
    "    data['Change in Close Price'] = data['Close'] - data['Close'].shift(1)\n",
    "    data['Scaled Volume'] = data['Volume']/data['Volume'].mean()\n",
    "    data_SMA = data['Adj Close'].rolling(window=3).mean().shift(1)\n",
    "    data['SMA(3)'] = data_SMA\n",
    "    data.reset_index(inplace=True)\n",
    "    data['Datetime']=data['Datetime'].dt.tz_convert('America/Montreal').dt.tz_localize(None)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news(ticker_code):\n",
    "    # 1. Define URL:\n",
    "    finwiz_url = 'https://finviz.com/quote.ashx?t='\n",
    "    # 2. Requesting data:\n",
    "    news_tables = {}\n",
    "    tickers = [ticker_code]\n",
    "    for ticker in tickers:\n",
    "        url = finwiz_url + ticker\n",
    "        req = Request(url=url,headers={'user-agent': 'my-app/0.0.1'}) \n",
    "        response = urlopen(req)    \n",
    "        # Read the contents of the file into 'html'\n",
    "        html = BeautifulSoup(response)\n",
    "        # Find 'news-table' in the Soup and load it into 'news_table'\n",
    "        news_table = html.find(id='news-table')\n",
    "        # Add the table to our dictionary\n",
    "        news_tables[ticker] = news_table\n",
    "    #3. Parsing news:\n",
    "    parsed_news = []\n",
    "    # Iterate through the news\n",
    "    for file_name, news_table in news_tables.items():\n",
    "        # Iterate through all tr tags in 'news_table'\n",
    "        for x in news_table.findAll('tr'):\n",
    "            # read the text from each tr tag into text\n",
    "            # get text from a only\n",
    "            text = x.a.get_text() \n",
    "            # splite text in the td tag into a list \n",
    "            date_scrape = x.td.text.split()\n",
    "            # if the length of 'date_scrape' is 1, load 'time' as the only element\n",
    "            if len(date_scrape) == 1:\n",
    "                time = date_scrape[0]\n",
    "            # else load 'date' as the 1st element and 'time' as the second    \n",
    "            else:\n",
    "                date = date_scrape[0]\n",
    "                time = date_scrape[1]\n",
    "            # Extract the ticker from the file name, get the string up to the 1st '_'  \n",
    "            ticker = file_name.split('_')[0]\n",
    "            # Append ticker, date, time and headline as a list to the 'parsed_news' list\n",
    "            parsed_news.append([ticker, date, time, text])\n",
    "\n",
    "    # 4. Split into columns and save:\n",
    "    vader = SentimentIntensityAnalyzer()\n",
    "    columns = ['ticker', 'date', 'time', 'headline']\n",
    "    # Convert the parsed_news list into a DataFrame called 'parsed_and_scored_news'\n",
    "    parsed_and_scored_news = pd.DataFrame(parsed_news, columns=columns)\n",
    "    # Iterate through the headlines and get the polarity scores using vader\n",
    "    scores = parsed_and_scored_news['headline'].apply(vader.polarity_scores).tolist()\n",
    "    # Convert the 'scores' list of dicts into a DataFrame\n",
    "    scores_df = pd.DataFrame(scores)\n",
    "    # Join the DataFrames of the news and the list of dicts\n",
    "    parsed_and_scored_news = parsed_and_scored_news.join(scores_df, rsuffix='_right')\n",
    "    parsed_and_scored_news.insert(loc=1, column='date_time', value=(pd.to_datetime(parsed_and_scored_news['date'] + ' ' + parsed_and_scored_news['time'])))\n",
    "    parsed_and_scored_news.drop(columns=['date','time'],axis=1,inplace=True)\n",
    "    return parsed_and_scored_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_for_hashtags(hashtag_phrase):\n",
    "    format_hashtag = '$'+hashtag_phrase\n",
    "    #auth = tweepy.OAuthHandler(\"psxikKIcDvu19SDl1qbTycOHY\", \"H2FV5HL4UyuzVUtANd0lo3HpsNzo1woDMuwlabVyv2E7g44SDb\")\n",
    "    #auth.set_access_token(\"1265371572942102528-XZyshdLemub7C7hrx51dyFrYCBCmvU\", \"CK4hmNukUYBhzr3CAtO3gvGFx7Ahr9TW3vZMjW6pl8yVd\")\n",
    "\n",
    "    #auth = tweepy.OAuthHandler(\"4BSOA2XKS8vucCHyBjy502Aw8\", \"l27Zsdh9X9oGGff25gJ4PX9zN6ZnjYKonu2zISu17jsQlO5Dkb\")\n",
    "    #auth.set_access_token(\"1265371572942102528-IsosHWjrXRDKHaqQFAbSPuM2FyH41k\", \"pVjYFAx8pDCBnJE48NxsA4KB6g4eNMw39TOWcTciLOB5u\")\n",
    "\n",
    "    #auth = tweepy.OAuthHandler(\"dQXGlGv8YFtPrZvQwcCnvbged\", \"ouKbQUg1dGubyKbp7DrNo45Qdv3nNzd7MyuvCgIKha0vpuNbDA\")\n",
    "    #auth.set_access_token(\"1265371572942102528-Jmu9hvd4yBep0KwV9U5mHFUnyUi9JV\", \"CYW3FRkyXJnSzRrHoN9FcLlBdSRHfA7GWJK1PMT7Q1S7P\")\n",
    "\n",
    "    auth = tweepy.OAuthHandler(\"ljU1UWBCC0YNKlO9pwm1TshUc\", \"w5CDP6fNeNQhvDs075KiZQEIWI7VY1Z8BxiDc5kUAHTsXzOhCY\")\n",
    "    auth.set_access_token(\"1309241898419322880-hBG39tjNql0FcHrjOMYg2qTN3TEtnw\", \"p0UOsVpTB9hu15R3YBMbWGw2zDpJjkUCmK0YadzA894ZF\")\n",
    "\n",
    "    api = tweepy.API(auth)\n",
    "   \n",
    "    twitter_posts = pd.DataFrame(columns=['timestamp', 'tweet_text', 'followers_count'])\n",
    "    timestamp=[]\n",
    "    tweets=[]\n",
    "    follow_count=[]\n",
    "    while True:\n",
    "        try:\n",
    "            for tweet in tweepy.Cursor(api.search, q=format_hashtag+' -filter:retweets', lang=\"en\", tweet_mode='extended').items():\n",
    "                timestamp.append(tweet.created_at)\n",
    "                tweets.append(tweet.full_text.replace('\\n',' ').encode('utf-8'))\n",
    "                follow_count.append(tweet.user.followers_count)\n",
    "        except tweepy.TweepError:\n",
    "            break\n",
    "        except StopIteration:\n",
    "            break\n",
    "    twitter_posts['timestamp']=timestamp\n",
    "    twitter_posts['tweet_text']=tweets\n",
    "    twitter_posts['followers_count']=follow_count\n",
    "      \n",
    "    return twitter_posts\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "2293\n"
    }
   ],
   "source": [
    "tweets = search_for_hashtags('AAPL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "               timestamp                                         tweet_text  \\\n0    2020-09-30 20:16:22  b&#39;#FAANG Stocks Overview: https://t.co/QEVM36K...   \n1    2020-09-30 20:15:49  b&quot;$AAPL pushes through Tuesday&#39;s high: https:/...   \n2    2020-09-30 20:15:40  b&quot;Can&#39;t Wait  $aapl $amzn $abbv $ba $brk $bhc ...   \n3    2020-09-30 20:15:00  b&#39;Get today winning alerts https://t.co/ou6BBH...   \n4    2020-09-30 20:15:00  b&#39;\\xf0\\x9f\\x94\\xb4\\xf0\\x9f\\x94\\xb4  You are in...   \n...                  ...                                                ...   \n2288 2020-09-29 21:14:54  b&#39;Most active Tuesday - $NIO $HUSA $AAPL $ADIL...   \n2289 2020-09-29 21:13:13  b&quot;I&#39;ve made  35k with them . If you  want to m...   \n2290 2020-09-29 21:12:55  b&quot;Most active stocks from today&#39;s after-hours ...   \n2291 2020-09-29 21:12:17  b&#39;@WarrenBuffett geico will acquire $lmnd 170%...   \n2292 2020-09-29 21:10:00  b&#39;$boxl $fb $blnk $aapl $shll $nkla $vxrt $evf...   \n\n      followers_count  \n0                1085  \n1                 853  \n2                 130  \n3                  18  \n4              143246  \n...               ...  \n2288            12330  \n2289                0  \n2290            11745  \n2291               29  \n2292             4253  \n\n[2293 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>timestamp</th>\n      <th>tweet_text</th>\n      <th>followers_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2020-09-30 20:16:22</td>\n      <td>b'#FAANG Stocks Overview: https://t.co/QEVM36K...</td>\n      <td>1085</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2020-09-30 20:15:49</td>\n      <td>b\"$AAPL pushes through Tuesday's high: https:/...</td>\n      <td>853</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2020-09-30 20:15:40</td>\n      <td>b\"Can't Wait  $aapl $amzn $abbv $ba $brk $bhc ...</td>\n      <td>130</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2020-09-30 20:15:00</td>\n      <td>b'Get today winning alerts https://t.co/ou6BBH...</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2020-09-30 20:15:00</td>\n      <td>b'\\xf0\\x9f\\x94\\xb4\\xf0\\x9f\\x94\\xb4  You are in...</td>\n      <td>143246</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2288</th>\n      <td>2020-09-29 21:14:54</td>\n      <td>b'Most active Tuesday - $NIO $HUSA $AAPL $ADIL...</td>\n      <td>12330</td>\n    </tr>\n    <tr>\n      <th>2289</th>\n      <td>2020-09-29 21:13:13</td>\n      <td>b\"I've made  35k with them . If you  want to m...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2290</th>\n      <td>2020-09-29 21:12:55</td>\n      <td>b\"Most active stocks from today's after-hours ...</td>\n      <td>11745</td>\n    </tr>\n    <tr>\n      <th>2291</th>\n      <td>2020-09-29 21:12:17</td>\n      <td>b'@WarrenBuffett geico will acquire $lmnd 170%...</td>\n      <td>29</td>\n    </tr>\n    <tr>\n      <th>2292</th>\n      <td>2020-09-29 21:10:00</td>\n      <td>b'$boxl $fb $blnk $aapl $shll $nkla $vxrt $evf...</td>\n      <td>4253</td>\n    </tr>\n  </tbody>\n</table>\n<p>2293 rows Ã— 3 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 101
    }
   ],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_change_sentiment(data, col):\n",
    "    change_in_sent = []\n",
    "    change_in_sent.append(data[col][0])\n",
    "    for i in range(1,len(data[col])):\n",
    "        if data[col][i] == 0:\n",
    "            change_in_sent.append(0)\n",
    "        elif data[col][i] < 0 or data[col][i] > 0:\n",
    "            dif = data[col][i] - data[col][(i-1)]\n",
    "            change_in_sent.append(dif)\n",
    "    return change_in_sent\n",
    "\n",
    "def remove_pattern(input_txt, pattern):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for i in r:\n",
    "        input_txt = re.sub(i, '', input_txt)       \n",
    "    return input_txt\n",
    "    \n",
    "def clean_tweets(tweets):\n",
    "    #remove twitter Return handles (RT @xxx:)\n",
    "    tweets = np.vectorize(remove_pattern)(tweets, \"RT @[\\w]*:\") \n",
    "    #remove twitter handles (@xxx)\n",
    "    tweets = np.vectorize(remove_pattern)(tweets, \"@[\\w]*\")\n",
    "    #remove URL links (httpxxx)\n",
    "    tweets = np.vectorize(remove_pattern)(tweets, \"https?://[A-Za-z0-9./]*\")\n",
    "    tweets = np.vectorize(remove_pattern)(tweets, \"b'\")\n",
    "    tweets = np.vectorize(remove_pattern)(tweets, 'b\"')\n",
    "    #remove special characters, numbers, punctuations (except for #)\n",
    "    tweets = np.core.defchararray.replace(tweets, \"[^a-zA-Z]\", \" \")\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_news(dataframe, datetime_column_name):\n",
    "    day1, day2, day3, day4= [],[],[],[]\n",
    "\n",
    "    for i in range(len(dataframe[datetime_column_name])):\n",
    "        if dataframe[datetime_column_name][i].day == dataframe[datetime_column_name][i].day and (dataframe[datetime_column_name][i].hour <= 15 and dataframe[datetime_column_name][i].hour >= 9):\n",
    "            day1.append(i)\n",
    "        elif dataframe[datetime_column_name][i].day == dataframe[datetime_column_name][i].day+1 and (dataframe[datetime_column_name][i].hour <= 15 and dataframe[datetime_column_name][i].hour >= 9):\n",
    "            day2.append(i)\n",
    "        elif dataframe[datetime_column_name][i].day == dataframe[datetime_column_name][i].day+2 and (dataframe[datetime_column_name][i].hour <= 15 and dataframe[datetime_column_name][i].hour >= 9):\n",
    "            day3.append(i)       \n",
    "        elif dataframe[datetime_column_name][i].day == dataframe[datetime_column_name][i].day+3 and (dataframe[datetime_column_name][i].hour <= 15 and dataframe[datetime_column_name][i].hour >= 9):\n",
    "            day4.append(i)\n",
    "        else:\n",
    "            pass\n",
    "    news_d1, news_d2,news_d3,news_d4 = dataframe.iloc[day1],dataframe.iloc[day2],dataframe.iloc[day3],dataframe.iloc[day4]\n",
    "    return news_d1, news_d2,news_d3,news_d4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_headlines(data):\n",
    "    data.drop_duplicates(subset='headline',keep=False, inplace=True)\n",
    "    data.drop('ticker', axis=1, inplace=True)\n",
    "    data.set_index('date_time', inplace=True)\n",
    "    data_30m = data.resample('30min').median().ffill().reset_index()\n",
    "    change_in_sent=calc_change_sentiment(data_30m, 'compound')\n",
    "    data_30m['change in sentiment headlines'] = change_in_sent\n",
    "    data_30m['change in sentiment headlines (t-1)'] = data_30m['change in sentiment headlines'].shift(1)\n",
    "\n",
    "    news_d1, news_d2,news_d3,news_d4 = classify_news(data_30m, 'date_time')\n",
    "    news_d1_red, news_d2_red,news_d3_red,news_d4_red = news_d1.iloc[1:],news_d2.iloc[1:],news_d3.iloc[1:],news_d4.iloc[1:]\n",
    "\n",
    "    frames_news = [news_d1_red, news_d2_red,news_d3_red,news_d4_red]\n",
    "    processed_headlines = pd.concat(frames_news)\n",
    "    return processed_headlines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_posts(dataframe):\n",
    "    vader = SentimentIntensityAnalyzer()\n",
    "    dataframe['tweet_text'] = clean_tweets(dataframe['tweet_text'])\n",
    "    scores = dataframe['tweet_text'].apply(vader.polarity_scores).tolist()\n",
    "    scores_df = pd.DataFrame(scores)\n",
    "\n",
    "    df = dataframe.join(scores_df, rsuffix='_right')\n",
    "    df = df[['timestamp','tweet_text','followers_count','neg','neu','pos','compound']]\n",
    "    df['timestamp'] = df['timestamp'].dt.tz_localize('UTC').dt.tz_convert('America/Montreal').dt.tz_localize(None)\n",
    "    df['scaled_followers_count'] =(df['followers_count']/df['followers_count'].max()) + 1\n",
    "    df['adj compound'] = df['compound']*df['scaled_followers_count']\n",
    "    df.set_index('timestamp', inplace=True)\n",
    "\n",
    "    twitter_df_30m = df.resample('30min').median().ffill().reset_index()\n",
    "    change_in_sent = calc_change_sentiment(twitter_df_30m, 'adj compound')\n",
    "    twitter_df_30m['change in sentiment twitter'] = change_in_sent\n",
    "    twitter_df_30m['change in sentiment twitter (t-1)'] = twitter_df_30m['change in sentiment twitter'].shift(1)\n",
    "\n",
    "    tweet_d1,tweet_d2,tweet_d3,tweet_d4 = classify_news(twitter_df_30m, 'timestamp')\n",
    "    tweet_d1_red,tweet_d2_red,tweet_d3_red,tweet_d4_red = tweet_d1.iloc[1:],tweet_d2.iloc[1:],tweet_d3.iloc[1:],tweet_d4.iloc[1:]\n",
    "\n",
    "    frames = [tweet_d1_red,tweet_d2_red,tweet_d3_red,tweet_d4_red]\n",
    "    processed_tweets = pd.concat(frames)\n",
    "    return processed_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(baseline_df, headline_df, twitter_df):\n",
    "    #1. Baseline:\n",
    "    baseline_rmse, baseline_r2 = baseline_model(baseline_df)\n",
    "    baseline_df2 = baseline_df\n",
    "    baseline_df2['t+1'] = baseline_df2['Adj Close'].shift(-1)\n",
    "    lm_baseline_rmse, lm_baseline_r2, sgd_baseline_rmse, sgd_baseline_r2 = linear_modeling_no_sentiment(baseline_df2)\n",
    "    #2. Headline Final Merge:\n",
    "    headlines_final = preprocess_headlines(headline_df)\n",
    "    with_headlines_df = stock_df.merge(headlines_final, left_on='Datetime', right_on='date_time').drop('date_time',axis=1)\n",
    "    with_headlines_df['t+1'] = with_headlines_df['Adj Close'].shift(-1)\n",
    "    #3. Twitter Final Merge:\n",
    "    final_twitter = preprocess_posts(twitter_df)\n",
    "    with_twitter_df = stock_df.merge(final_twitter, left_on='Datetime', right_on='timestamp').drop('timestamp',axis=1)\n",
    "    with_twitter_df['t+1'] = with_twitter_df['Adj Close'].shift(-1)\n",
    "    #4. Full Merge:\n",
    "    full_df = with_twitter_df.merge(headlines_final, left_on='Datetime', right_on='date_time').drop('date_time',axis=1)\n",
    "    full_df['t+1'] = full_df['Adj Close'].shift(-1)\n",
    "    #5. Evaluating Models:\n",
    "    lm_headlines_rmse, lm_headlines_r2, sgd_headlines_rmse, sgd_headlines_r2,xgb_headlines_rmse,xgb_headlines_r2 = linear_modeling_headlines(with_headlines_df)\n",
    "    lm_twitter_rmse, lm_twitter_r2, sgd_twitter_rmse, sgd_twitter_r2,xgb_twitter_rmse,xgb_twitter_r2 = linear_model_twitter(with_twitter_df)\n",
    "    lm_all_rmse, lm_all_r2, sgd_all_rmse, sgd_all_r2, xgb_all_rmse, xgb_all_r2, rf_all_rmse, rf_all_r2 = multi_model_full(full_df)\n",
    "    #6. Store in dict:\n",
    "    result_dict = {\n",
    "    'RMSE - Baseline':baseline_rmse, 'R2 - Baseline':baseline_r2, 'Linear RMSE - Baseline':lm_baseline_rmse, 'Linear R2 - Baseline':lm_baseline_r2, 'SGD RMSE - Baseline':sgd_baseline_rmse, 'SGD R2 - Baseline':sgd_baseline_r2,\n",
    "    'Linear RMSE - Only Headlines': lm_headlines_rmse, 'Linear R2 - Only Headlines':lm_headlines_r2, 'SGD RMSE - Only Headlines':sgd_headlines_rmse, 'SGD R2 - Only Headlines':sgd_headlines_r2, 'XGB RMSE - Only Headlines':xgb_headlines_rmse, 'XGB R2 - Only Headlines':xgb_headlines_r2,\n",
    "    'Linear RMSE - Only Twitter':lm_twitter_rmse, 'Linear R2 - Only Twitter':lm_twitter_r2, 'SGD RMSE - Only Twitter':sgd_twitter_rmse, 'SGD R2 - Only Twitter':sgd_twitter_r2, 'XGB RMSE - Only Twitter':xgb_twitter_rmse, 'XGB R2 - Only Twitter':xgb_twitter_r2,\n",
    "    'Linear RMSE - All':lm_all_rmse, 'Linear R2 - All':lm_all_r2, 'SGD RMSE - All':sgd_all_rmse, 'SGD R2 - All':sgd_all_r2, 'XGB RMSE - All':xgb_all_rmse, 'XGB R2 - All':xgb_all_r2, 'RF RMSE - All':rf_all_rmse,'RF R2 - All':rf_all_r2}\n",
    "    #7. Convert to DataFrame:\n",
    "    result_df = pd.DataFrame.from_dict(result_dict, orient='index', columns=['Values'])\n",
    "    #result_df.to_csv('~/LighthouseLabs-Final/Report_Analysis/AAPL_complete_analysis.csv')\n",
    "    return result_df, full_df"
   ]
  }
 ]
}