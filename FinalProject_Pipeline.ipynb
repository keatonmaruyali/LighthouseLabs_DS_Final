{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.9 64-bit ('base': conda)",
   "display_name": "Python 3.7.9 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "73a666d948711064f59d854e9125f239b700b6d583285c0d9848266fb8325020"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import re\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "source": [
    "## 1. Defining Functions:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 1.1 Preprocessing Functions:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_change_sentiment(data, col):\n",
    "    change_in_sent = []\n",
    "    change_in_sent.append(data[col][0])\n",
    "    for i in range(1,len(data[col])):\n",
    "        if data[col][i] == 0:\n",
    "            change_in_sent.append(0)\n",
    "        elif data[col][i] < 0 or data[col][i] > 0:\n",
    "            dif = data[col][i] - data[col][(i-1)]\n",
    "            change_in_sent.append(dif)\n",
    "    return change_in_sent\n",
    "\n",
    "def remove_pattern(input_txt, pattern):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for i in r:\n",
    "        input_txt = re.sub(i, '', input_txt)       \n",
    "    return input_txt\n",
    "    \n",
    "def clean_tweets(tweets):\n",
    "    #remove twitter Return handles (RT @xxx:)\n",
    "    tweets = np.vectorize(remove_pattern)(tweets, \"RT @[\\w]*:\") \n",
    "    \n",
    "    #remove twitter handles (@xxx)\n",
    "    tweets = np.vectorize(remove_pattern)(tweets, \"@[\\w]*\")\n",
    "    \n",
    "    #remove URL links (httpxxx)\n",
    "    tweets = np.vectorize(remove_pattern)(tweets, \"https?://[A-Za-z0-9./]*\")\n",
    "    \n",
    "    tweets = np.vectorize(remove_pattern)(tweets, \"b'\")\n",
    "    tweets = np.vectorize(remove_pattern)(tweets, 'b\"')\n",
    "\n",
    "\n",
    "    #remove special characters, numbers, punctuations (except for #)\n",
    "    tweets = np.core.defchararray.replace(tweets, \"[^a-zA-Z]\", \" \")\n",
    "    \n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_news(dataframe, datetime_column_name):\n",
    "\n",
    "    day22, day23, day24, day25, day26, day27, day28, day29, day30, day31, day32, day33, day34 = [],[],[],[],[],[],[],[],[],[],[],[],[]\n",
    "\n",
    "    for i in range(len(dataframe[datetime_column_name])):\n",
    "        #if dataframe[datetime_column_name][i].day == 21 and dataframe[datetime_column_name][i].hour > 17:day22.append(i)\n",
    "        if dataframe[datetime_column_name][i].day == 22 and (dataframe[datetime_column_name][i].hour <= 15 and dataframe[datetime_column_name][i].hour >= 9):\n",
    "            day22.append(i)\n",
    "        #if dataframe[datetime_column_name][i].day == 22 and dataframe[datetime_column_name][i].hour > 17:day23.append(i)\n",
    "        elif dataframe[datetime_column_name][i].day == 23 and (dataframe[datetime_column_name][i].hour <= 15 and dataframe[datetime_column_name][i].hour >= 9):\n",
    "            day23.append(i)\n",
    "        #if dataframe[datetime_column_name][i].day == 23 and dataframe[datetime_column_name][i].hour > 17:day24.append(i)\n",
    "        elif dataframe[datetime_column_name][i].day == 24 and (dataframe[datetime_column_name][i].hour <= 15 and dataframe[datetime_column_name][i].hour >= 9):\n",
    "            day24.append(i)       \n",
    "        #if dataframe[datetime_column_name][i].day == 24 and dataframe[datetime_column_name][i].hour > 17:day25.append(i)\n",
    "        elif dataframe[datetime_column_name][i].day == 25 and (dataframe[datetime_column_name][i].hour <= 15 and dataframe[datetime_column_name][i].hour >= 9):\n",
    "            day25.append(i)\n",
    "        #if dataframe[datetime_column_name][i].day == 25 and dataframe[datetime_column_name][i].hour > 17:day26.append(i)\n",
    "        elif dataframe[datetime_column_name][i].day == 26 and (dataframe[datetime_column_name][i].hour <= 15 and dataframe[datetime_column_name][i].hour >= 9):\n",
    "            day26.append(i)\n",
    "        #if dataframe[datetime_column_name][i].day == 26 and dataframe[datetime_column_name][i].hour > 17:day27.append(i)\n",
    "        elif dataframe[datetime_column_name][i].day == 27 and (dataframe[datetime_column_name][i].hour <= 15 and dataframe[datetime_column_name][i].hour >= 9):\n",
    "            day27.append(i)\n",
    "        #if dataframe[datetime_column_name][i].day == 27 and dataframe[datetime_column_name][i].hour > 17:day28.append(i)\n",
    "        elif dataframe[datetime_column_name][i].day == 28 and (dataframe[datetime_column_name][i].hour <= 15 and dataframe[datetime_column_name][i].hour >= 9):\n",
    "            day28.append(i)\n",
    "        #if dataframe[datetime_column_name][i].day == 28 and dataframe[datetime_column_name][i].hour > 17:day29.append(i)\n",
    "        elif dataframe[datetime_column_name][i].day == 29 and (dataframe[datetime_column_name][i].hour <= 15 and dataframe[datetime_column_name][i].hour >= 9):\n",
    "            day29.append(i)\n",
    "        #if dataframe[datetime_column_name][i].day == 29 and dataframe[datetime_column_name][i].hour > 17:day30.append(i)\n",
    "        elif dataframe[datetime_column_name][i].day == 30 and (dataframe[datetime_column_name][i].hour <= 15 and dataframe[datetime_column_name][i].hour >= 9):\n",
    "            day30.append(i)\n",
    "        elif dataframe[datetime_column_name][i].day == 1 and (dataframe[datetime_column_name][i].hour <= 15 and dataframe[datetime_column_name][i].hour >= 9):\n",
    "            day31.append(i)\n",
    "        elif dataframe[datetime_column_name][i].day == 2 and (dataframe[datetime_column_name][i].hour <= 15 and dataframe[datetime_column_name][i].hour >= 9):\n",
    "            day32.append(i)\n",
    "        elif dataframe[datetime_column_name][i].day == 3 and (dataframe[datetime_column_name][i].hour <= 15 and dataframe[datetime_column_name][i].hour >= 9):\n",
    "            day33.append(i)\n",
    "        elif dataframe[datetime_column_name][i].day == 4 and (dataframe[datetime_column_name][i].hour <= 15 and dataframe[datetime_column_name][i].hour >= 9):\n",
    "            day34.append(i)\n",
    "\n",
    "        else:\n",
    "            pass\n",
    "    news_d22, news_d23,news_d24,news_d25,news_d26,news_d27,news_d28,news_d29,news_d30,news_d31,news_d32,news_d33,news_d34 = dataframe.iloc[day22],dataframe.iloc[day23],dataframe.iloc[day24],dataframe.iloc[day25], dataframe.iloc[day26], dataframe.iloc[day27],dataframe.iloc[day28],dataframe.iloc[day29],dataframe.iloc[day30],dataframe.iloc[day31], dataframe.iloc[day32],dataframe.iloc[day33],dataframe.iloc[day34]\n",
    "    return news_d22, news_d23,news_d24,news_d25,news_d26,news_d27,news_d28,news_d29,news_d30,news_d31,news_d32,news_d33,news_d34\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_headlines(data):\n",
    "    data.drop_duplicates(subset='headline',keep=False, inplace=True)\n",
    "    data.drop('ticker', axis=1, inplace=True)\n",
    "    data.set_index('date_time', inplace=True)\n",
    "    data_30m = data.resample('30min').median().ffill().reset_index()\n",
    "    change_in_sent=calc_change_sentiment(data_30m, 'compound')\n",
    "    data_30m['change in sentiment headlines'] = change_in_sent\n",
    "    data_30m['change in sentiment headlines (t-1)'] = data_30m['change in sentiment headlines'].shift(1)\n",
    "\n",
    "    news_d22, news_d23,news_d24,news_d25,news_d26,news_d27,news_d28,news_d29,news_d30,news_d31,news_d32,news_d33,news_d34 = classify_news(data_30m, 'date_time')\n",
    "\n",
    "    news_d23_red,news_d24_red,news_d25_red,news_d26_red,news_d27_red,news_d28_red,news_d29_red,news_d30_red,news_d31_red,news_d32_red,news_d33_red,news_d34_red = news_d23.iloc[4:],news_d24.iloc[1:],news_d25.iloc[1:],news_d26.iloc[1:],news_d27.iloc[1:],news_d28.iloc[1:],news_d29.iloc[1:],news_d30.iloc[1:],news_d31.iloc[1:],news_d32.iloc[1:],news_d33.iloc[1:],news_d34.iloc[1:]\n",
    "\n",
    "    frames_news = [news_d23_red,news_d24_red, news_d25_red, news_d28_red]\n",
    "    netflix_headlines_30m_d23_24_25 = pd.concat(frames_news)\n",
    "    return netflix_headlines_30m_d23_24_25\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_posts(dataframe):\n",
    "    vader = SentimentIntensityAnalyzer()\n",
    "    dataframe['tweet_text'] = clean_tweets(dataframe['tweet_text'])\n",
    "    scores = dataframe['tweet_text'].apply(vader.polarity_scores).tolist()\n",
    "    scores_df = pd.DataFrame(scores)\n",
    "\n",
    "    df = dataframe.join(scores_df, rsuffix='_right')\n",
    "    df = df[['timestamp','tweet_text','followers_count','neg','neu','pos','compound']]\n",
    "    df['timestamp'] = df['timestamp'].dt.tz_localize('UTC').dt.tz_convert('America/Montreal').dt.tz_localize(None)\n",
    "    df['scaled_followers_count'] =(df['followers_count']/df['followers_count'].max()) + 1\n",
    "    df['adj compound'] = df['compound']*df['scaled_followers_count']\n",
    "    df.set_index('timestamp', inplace=True)\n",
    "\n",
    "    twitter_df_30m = df.resample('30min').median().ffill().reset_index()\n",
    "    change_in_sent = calc_change_sentiment(twitter_df_30m, 'adj compound')\n",
    "    twitter_df_30m['change in sentiment twitter'] = change_in_sent\n",
    "    twitter_df_30m['change in sentiment twitter (t-1)'] = twitter_df_30m['change in sentiment twitter'].shift(1)\n",
    "\n",
    "    tw_news_d22,tw_news_d23,tw_news_d24,tw_news_d25,tw_news_d26,tw_news_d27,tw_news_d28,tw_news_d29,tw_news_d30,tw_news_d31,tw_news_d32,tw_news_d33,tw_news_d34 = classify_news(twitter_df_30m, 'timestamp')\n",
    "\n",
    "    tw_news_d23_30m,tw_news_d24_30m,tw_news_d25_30m, tw_news_d26_30m,tw_news_d27_30m,tw_news_d28_30m,tw_news_d29_30m,tw_news_d30_30m,tw_news_d31_30m,tw_news_d32_30m,tw_news_d33_30m,tw_news_d34_30m = tw_news_d23.iloc[4:],tw_news_d24.iloc[1:],tw_news_d25.iloc[1:],tw_news_d26.iloc[1:],tw_news_d27.iloc[1:],tw_news_d28.iloc[1:],tw_news_d29.iloc[1:],tw_news_d30.iloc[1:],tw_news_d31.iloc[1:],tw_news_d32.iloc[1:],tw_news_d33.iloc[1:],tw_news_d34.iloc[1:]\n",
    "\n",
    "    frames = [tw_news_d23_30m,tw_news_d24_30m,tw_news_d25_30m, tw_news_d28_30m,tw_news_d29_30m,tw_news_d30_30m,tw_news_d31_30m,tw_news_d32_30m,tw_news_d33_30m,tw_news_d34_30m]\n",
    "    processed_tweets = pd.concat(frames)\n",
    "    return processed_tweets"
   ]
  },
  {
   "source": [
    "### 1.2 Modeling Functions:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_model(data):\n",
    "    # data_SMA = data['Adj Close'].rolling(window=3).mean().shift(1)\n",
    "    # data['SMA(3)'] = data_SMA\n",
    "    pred = data['SMA(3)'][3:]\n",
    "    actu = data['Adj Close'][3:]\n",
    "    rmse = np.sqrt(mean_squared_error(actu,pred))\n",
    "    r2_sco = r2_score(actu,pred)\n",
    "    # print('Root Mean Squared Error: ',rmse)\n",
    "    # print('R2 Score: ', r2_sco)\n",
    "    return rmse, r2_sco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_modeling_no_sentiment(dataframe):\n",
    "    i = len(dataframe['t+1'])-4\n",
    "    y_train, y_test = dataframe['t+1'][3:i], dataframe['t+1'][i:-1]\n",
    "    X_train, X_test = dataframe[['Adj Close','Scaled Volume','SMA(3)']][3:i], dataframe[['Adj Close','Scaled Volume','SMA(3)']][i:-1]\n",
    "\n",
    "    lm = LinearRegression()\n",
    "    lm.fit(X_train,y_train)\n",
    "    predictions = lm.predict(X_test)\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(y_test,predictions))\n",
    "    r2_sco = r2_score(y_test,predictions)\n",
    "    # print('LR Root Mean Squared Error: ',rmse)\n",
    "    # print('LR R2 Score: ', r2_sco, '\\n')\n",
    "    \n",
    "    reg = SGDRegressor(random_state=42)\n",
    "    reg.fit(X_train, y_train)\n",
    "    predictions2 = reg.predict(X_test)\n",
    "    rmse2 = np.sqrt(mean_squared_error(y_test,predictions2))\n",
    "    r2_sco2 = r2_score(y_test,predictions2)\n",
    "    # print('SGD Root Mean Squared Error: ',rmse2)\n",
    "    # print('SGD R2 Score: ', r2_sco2)\n",
    "    return rmse,r2_sco,rmse2,r2_sco2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_modeling_headlines(dataframe):\n",
    "    i = len(dataframe['t+1'])-4\n",
    "    y_train, y_test = dataframe['t+1'][:i], dataframe['t+1'][i:-1]\n",
    "    X_train, X_test = dataframe[['Adj Close','Scaled Volume','compound','SMA(3)','change in sentiment headlines','change in sentiment headlines (t-1)']][:i], dataframe[['Adj Close','Scaled Volume','compound','SMA(3)','change in sentiment headlines','change in sentiment headlines (t-1)']][i:-1]\n",
    "\n",
    "    lm = LinearRegression()\n",
    "    lm.fit(X_train,y_train)\n",
    "    predictions = lm.predict(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test,predictions))\n",
    "    r2_sco = r2_score(y_test,predictions)\n",
    "    # print('LR Root Mean Squared Error: ',rmse)\n",
    "    # print('LR R2 Score: ', r2_sco,'\\n')\n",
    "    \n",
    "    reg = SGDRegressor(random_state=42)\n",
    "    reg.fit(X_train, y_train)\n",
    "    predictions2 = reg.predict(X_test)\n",
    "    rmse2 = np.sqrt(mean_squared_error(y_test,predictions2))\n",
    "    r2_sco2 = r2_score(y_test,predictions2)\n",
    "    # print('SGD Root Mean Squared Error: ',rmse2)\n",
    "    # print('SGD R2 Score: ', r2_sco2)\n",
    "\n",
    "    xg_reg = xgb.XGBRegressor(booster='gblinear', learning_rate = 0.03, n_estimators = 10000)\n",
    "    xg_reg.fit(X_train,y_train)\n",
    "    preds3 = xg_reg.predict(X_test)\n",
    "    rmse3 = np.sqrt(mean_squared_error(y_test, preds3))\n",
    "    r2_sco3 = r2_score(y_test,preds3)\n",
    "    \n",
    "    return rmse,r2_sco,rmse2,r2_sco2,rmse3,r2_sco3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_model_twitter(dataframe):\n",
    "    i = len(dataframe['t+1'])-4\n",
    "    y_train, y_test = dataframe['t+1'][:i], dataframe['t+1'][i:-1]\n",
    "    X_train, X_test = dataframe[['Adj Close','Scaled Volume','compound','SMA(3)','change in sentiment twitter','change in sentiment twitter (t-1)']][:i], dataframe[['Adj Close','Scaled Volume','compound','SMA(3)','change in sentiment twitter','change in sentiment twitter (t-1)']][i:-1]\n",
    "\n",
    "    lm = LinearRegression()\n",
    "    lm.fit(X_train,y_train)\n",
    "    predictions = lm.predict(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test,predictions))\n",
    "    r2_sco = r2_score(y_test,predictions)\n",
    "    # print('LR Root Mean Squared Error: ',rmse)\n",
    "    # print('LR R2 Score: ', r2_sco,'\\n')\n",
    "\n",
    "    reg = SGDRegressor(random_state=42)\n",
    "    reg.fit(X_train, y_train)\n",
    "    predictions2 = reg.predict(X_test)\n",
    "    rmse2 = np.sqrt(mean_squared_error(y_test,predictions2))\n",
    "    r2_sco2 = r2_score(y_test,predictions2)\n",
    "    # print('SGD Root Mean Squared Error: ',rmse2)\n",
    "    # print('SGD R2 Score: ', r2_sco2)\n",
    "\n",
    "    xg_reg = xgb.XGBRegressor(booster='gblinear', learning_rate = 0.03, n_estimators = 10000)\n",
    "    xg_reg.fit(X_train,y_train)\n",
    "    preds3 = xg_reg.predict(X_test)\n",
    "    rmse3 = np.sqrt(mean_squared_error(y_test, preds3))\n",
    "    r2_sco3 = r2_score(y_test,preds3)\n",
    "\n",
    "    return rmse,r2_sco,rmse2,r2_sco2,rmse3,r2_sco3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_model_full(dataframe):\n",
    "    i = len(dataframe['t+1'])-4\n",
    "    y_train, y_test = dataframe['t+1'][:i], dataframe['t+1'][i:-1]\n",
    "    X_train, X_test = dataframe[['Adj Close','Scaled Volume','compound_y','compound_x','SMA(3)','change in sentiment headlines','change in sentiment headlines (t-1)','change in sentiment twitter','change in sentiment twitter (t-1)']][:i], dataframe[['Adj Close','Scaled Volume','compound_y','compound_x','SMA(3)','change in sentiment headlines','change in sentiment headlines (t-1)','change in sentiment twitter','change in sentiment twitter (t-1)']][i:-1]\n",
    "\n",
    "    lm = LinearRegression()\n",
    "    lm.fit(X_train,y_train)\n",
    "    predictions = lm.predict(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test,predictions))\n",
    "    r2_sco = r2_score(y_test,predictions)\n",
    "    #print('LR Root Mean Squared Error: ',rmse)\n",
    "    #print('LR R2 Score: ', r2_sco,'\\n')\n",
    "\n",
    "    reg = SGDRegressor(random_state=42)\n",
    "    reg.fit(X_train, y_train)\n",
    "    predictions2 = reg.predict(X_test)\n",
    "    rmse2 = np.sqrt(mean_squared_error(y_test,predictions2))\n",
    "    r2_sco2 = r2_score(y_test,predictions2)\n",
    "    #print('SGD Root Mean Squared Error: ',rmse2)\n",
    "    #print('SGD R2 Score: ', r2_sco2,'\\n')\n",
    "\n",
    "    xg_reg = xgb.XGBRegressor(booster='gblinear', learning_rate = 0.03, n_estimators = 10000)\n",
    "    xg_reg.fit(X_train,y_train)\n",
    "    preds3 = xg_reg.predict(X_test)\n",
    "    rmse3 = np.sqrt(mean_squared_error(y_test, preds3))\n",
    "    r2_sco3 = r2_score(y_test,preds3)\n",
    "    #print('XGB Mean Squared Error: ',rmse3)\n",
    "    #print('XGB R2 Score: ', r2_sco3)\n",
    "\n",
    "    rf_regr = RandomForestRegressor(n_estimators=20, max_depth=600, random_state=42)\n",
    "    rf_regr.fit(X_train,y_train)\n",
    "    preds4 = rf_regr.predict(X_test)\n",
    "    rmse4 = np.sqrt(mean_squared_error(y_test, preds4))\n",
    "    r2_sco4 = r2_score(y_test,preds4)\n",
    "    return rmse,r2_sco,rmse2,r2_sco2,rmse3,r2_sco3,rmse4,r2_sco4"
   ]
  },
  {
   "source": [
    "## 2. Evaluate Model with Individual Stocks:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data(ticker):    \n",
    "    # 1. Historical Stock Data:\n",
    "    stock_df = pd.read_csv('~/LighthouseLabs-Final/1. Stock_Data/'+ticker+'_data.csv', parse_dates=['Datetime'])\n",
    "    # 2. Headline Data:\n",
    "    headlines1 = pd.read_csv('~/LighthouseLabs-Final/2. FinViz_Headline_Data/'+ticker+'_2020-09-23_2020-09-29.csv', index_col=0, parse_dates=['date_time'])\n",
    "    headlines2 = pd.read_csv('~/LighthouseLabs-Final/2. FinViz_Headline_Data/'+ticker+'_data_2020-09-30-20-43.csv', index_col=1, parse_dates=[['date','time']])\n",
    "    frames = [headlines1, headlines2]\n",
    "    headlines_df = pd.concat(frames)\n",
    "    headlines_df.drop_duplicates(subset='headline',keep='first',inplace=True)\n",
    "    headlines_df.to_csv('~/LighthouseLabs-Final/2. FinViz_Headline_Data/'+ticker+'_2020-09-23_2020-09-30.csv')\n",
    "\n",
    "    # 3. Twitter Data:\n",
    "    twitter1 = pd.read_csv('~/LighthouseLabs-Final/3. Twitter_Data/'+ticker+'_2020-09-23_2020-09-29.csv', parse_dates=['timestamp'])\n",
    "    twitter2 = pd.read_csv('~/LighthouseLabs-Final/3. Twitter_Data/'+ticker+'_2020-09-30.csv', parse_dates=['timestamp'])\n",
    "    #twitter3 = pd.read_csv('~/LighthouseLabs-Final/3. Twitter_Data/'+ticker+'_2020-09-30_2.csv', parse_dates=['timestamp'])\n",
    "    frames = [twitter1,twitter2]\n",
    "    twitter_df = pd.concat(frames)\n",
    "    twitter_df.drop_duplicates(subset='tweet_text',keep='first', inplace=True)\n",
    "    twitter_df.to_csv('~/LighthouseLabs-Final/3. Twitter_Data/'+ticker+'_2020-09-23_2020-09-30.csv')\n",
    "\n",
    "    return stock_df,headlines_df,twitter_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_models(baseline_df, headline_df, twitter_df):\n",
    "    #1. Baseline:\n",
    "    baseline_rmse, baseline_r2 = baseline_model(baseline_df)\n",
    "    baseline_df2 = baseline_df\n",
    "    baseline_df2['t+1'] = baseline_df2['Adj Close'].shift(-1)\n",
    "    lm_baseline_rmse, lm_baseline_r2, sgd_baseline_rmse, sgd_baseline_r2 = linear_modeling_no_sentiment(baseline_df2)\n",
    "    #2. Headline Final Merge:\n",
    "    headlines_final = preprocess_headlines(headline_df)\n",
    "    with_headlines_df = stock_df.merge(headlines_final, left_on='Datetime', right_on='date_time').drop('date_time',axis=1)\n",
    "    with_headlines_df['t+1'] = with_headlines_df['Adj Close'].shift(-1)\n",
    "    #3. Twitter Final Merge:\n",
    "    final_twitter = preprocess_posts(twitter_df)\n",
    "    with_twitter_df = stock_df.merge(final_twitter, left_on='Datetime', right_on='timestamp').drop('timestamp',axis=1)\n",
    "    with_twitter_df['t+1'] = with_twitter_df['Adj Close'].shift(-1)\n",
    "    #4. Full Merge:\n",
    "    full_df = with_twitter_df.merge(headlines_final, left_on='Datetime', right_on='date_time').drop('date_time',axis=1)\n",
    "    full_df['t+1'] = full_df['Adj Close'].shift(-1)\n",
    "    #5. Evaluating Models:\n",
    "    lm_headlines_rmse, lm_headlines_r2, sgd_headlines_rmse, sgd_headlines_r2,xgb_headlines_rmse,xgb_headlines_r2 = linear_modeling_headlines(with_headlines_df)\n",
    "    lm_twitter_rmse, lm_twitter_r2, sgd_twitter_rmse, sgd_twitter_r2,xgb_twitter_rmse,xgb_twitter_r2 = linear_model_twitter(with_twitter_df)\n",
    "    lm_all_rmse, lm_all_r2, sgd_all_rmse, sgd_all_r2, xgb_all_rmse, xgb_all_r2, rf_all_rmse, rf_all_r2 = multi_model_full(full_df)\n",
    "    #6. Store in dict:\n",
    "    result_dict = {\n",
    "    'RMSE - Baseline':baseline_rmse, 'R2 - Baseline':baseline_r2, 'Linear RMSE - Baseline':lm_baseline_rmse, 'Linear R2 - Baseline':lm_baseline_r2, 'SGD RMSE - Baseline':sgd_baseline_rmse, 'SGD R2 - Baseline':sgd_baseline_r2,\n",
    "    'Linear RMSE - Only Headlines': lm_headlines_rmse, 'Linear R2 - Only Headlines':lm_headlines_r2, 'SGD RMSE - Only Headlines':sgd_headlines_rmse, 'SGD R2 - Only Headlines':sgd_headlines_r2, 'XGB RMSE - Only Headlines':xgb_headlines_rmse, 'XGB R2 - Only Headlines':xgb_headlines_r2,\n",
    "    'Linear RMSE - Only Twitter':lm_twitter_rmse, 'Linear R2 - Only Twitter':lm_twitter_r2, 'SGD RMSE - Only Twitter':sgd_twitter_rmse, 'SGD R2 - Only Twitter':sgd_twitter_r2, 'XGB RMSE - Only Twitter':xgb_twitter_rmse, 'XGB R2 - Only Twitter':xgb_twitter_r2,\n",
    "    'Linear RMSE - All':lm_all_rmse, 'Linear R2 - All':lm_all_r2, 'SGD RMSE - All':sgd_all_rmse, 'SGD R2 - All':sgd_all_r2, 'XGB RMSE - All':xgb_all_rmse, 'XGB R2 - All':xgb_all_r2, 'RF RMSE - All':rf_all_rmse,'RF R2 - All':rf_all_r2}\n",
    "    #7. Convert to DataFrame:\n",
    "    result_df = pd.DataFrame.from_dict(result_dict, orient='index', columns=['Values'])\n",
    "    #result_df.to_csv('~/LighthouseLabs-Final/Report_Analysis/AAPL_complete_analysis.csv')\n",
    "    return result_df, full_df"
   ]
  },
  {
   "source": [
    "stock_df, headlines_df, twitter_df = import_data('NKE')\n",
    "result_df, full_df = evaluate_models(stock_df, headlines_df, twitter_df)\n",
    "result_df"
   ],
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "execution_count": 336,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                    Values\nRMSE - Baseline               7.534147e-01\nR2 - Baseline                 6.975580e-01\nLinear RMSE - Baseline        3.885663e-01\nLinear R2 - Baseline         -1.955353e-01\nSGD RMSE - Baseline           3.222889e+13\nSGD R2 - Baseline            -8.224753e+27\nLinear RMSE - Only Headlines  5.527221e-01\nLinear R2 - Only Headlines   -6.492233e-01\nSGD RMSE - Only Headlines     2.899242e+12\nSGD R2 - Only Headlines      -4.537688e+25\nXGB RMSE - Only Headlines     4.822432e-01\nXGB R2 - Only Headlines      -2.554460e-01\nLinear RMSE - Only Twitter    3.761056e-01\nLinear R2 - Only Twitter     -1.200872e-01\nSGD RMSE - Only Twitter       2.944792e+13\nSGD R2 - Only Twitter        -6.866594e+27\nXGB RMSE - Only Twitter       3.581040e-01\nXGB R2 - Only Twitter        -1.543122e-02\nLinear RMSE - All             5.638129e-01\nLinear R2 - All              -7.160730e-01\nSGD RMSE - All                2.384237e+12\nSGD R2 - All                 -3.068772e+25\nXGB RMSE - All                5.121096e-01\nXGB R2 - All                 -4.157665e-01\nRF RMSE - All                 8.808563e-01\nRF R2 - All                  -3.188668e+00",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Values</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>RMSE - Baseline</th>\n      <td>7.534147e-01</td>\n    </tr>\n    <tr>\n      <th>R2 - Baseline</th>\n      <td>6.975580e-01</td>\n    </tr>\n    <tr>\n      <th>Linear RMSE - Baseline</th>\n      <td>3.885663e-01</td>\n    </tr>\n    <tr>\n      <th>Linear R2 - Baseline</th>\n      <td>-1.955353e-01</td>\n    </tr>\n    <tr>\n      <th>SGD RMSE - Baseline</th>\n      <td>3.222889e+13</td>\n    </tr>\n    <tr>\n      <th>SGD R2 - Baseline</th>\n      <td>-8.224753e+27</td>\n    </tr>\n    <tr>\n      <th>Linear RMSE - Only Headlines</th>\n      <td>5.527221e-01</td>\n    </tr>\n    <tr>\n      <th>Linear R2 - Only Headlines</th>\n      <td>-6.492233e-01</td>\n    </tr>\n    <tr>\n      <th>SGD RMSE - Only Headlines</th>\n      <td>2.899242e+12</td>\n    </tr>\n    <tr>\n      <th>SGD R2 - Only Headlines</th>\n      <td>-4.537688e+25</td>\n    </tr>\n    <tr>\n      <th>XGB RMSE - Only Headlines</th>\n      <td>4.822432e-01</td>\n    </tr>\n    <tr>\n      <th>XGB R2 - Only Headlines</th>\n      <td>-2.554460e-01</td>\n    </tr>\n    <tr>\n      <th>Linear RMSE - Only Twitter</th>\n      <td>3.761056e-01</td>\n    </tr>\n    <tr>\n      <th>Linear R2 - Only Twitter</th>\n      <td>-1.200872e-01</td>\n    </tr>\n    <tr>\n      <th>SGD RMSE - Only Twitter</th>\n      <td>2.944792e+13</td>\n    </tr>\n    <tr>\n      <th>SGD R2 - Only Twitter</th>\n      <td>-6.866594e+27</td>\n    </tr>\n    <tr>\n      <th>XGB RMSE - Only Twitter</th>\n      <td>3.581040e-01</td>\n    </tr>\n    <tr>\n      <th>XGB R2 - Only Twitter</th>\n      <td>-1.543122e-02</td>\n    </tr>\n    <tr>\n      <th>Linear RMSE - All</th>\n      <td>5.638129e-01</td>\n    </tr>\n    <tr>\n      <th>Linear R2 - All</th>\n      <td>-7.160730e-01</td>\n    </tr>\n    <tr>\n      <th>SGD RMSE - All</th>\n      <td>2.384237e+12</td>\n    </tr>\n    <tr>\n      <th>SGD R2 - All</th>\n      <td>-3.068772e+25</td>\n    </tr>\n    <tr>\n      <th>XGB RMSE - All</th>\n      <td>5.121096e-01</td>\n    </tr>\n    <tr>\n      <th>XGB R2 - All</th>\n      <td>-4.157665e-01</td>\n    </tr>\n    <tr>\n      <th>RF RMSE - All</th>\n      <td>8.808563e-01</td>\n    </tr>\n    <tr>\n      <th>RF R2 - All</th>\n      <td>-3.188668e+00</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 336
    }
   ]
  },
  {
   "source": [
    "### 2.1 Save Model Report:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "result_df.to_csv('~/LighthouseLabs-Final/Report_Analysis/NKE_5d_complete_analysis.csv')\n",
    "print('Saved!')"
   ],
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "execution_count": 337,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Saved!\n"
    }
   ]
  },
  {
   "source": [
    "## 3. Evaluate Model with Multiple Stocks:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Historical Stock Data:\n",
    "stock_df1 = pd.read_csv('~/LighthouseLabs-Final/1. Stock_Data/TSLA_data.csv', parse_dates=['Datetime'])\n",
    "stock_df2 = pd.read_csv('~/LighthouseLabs-Final/1. Stock_Data/AMZN_data.csv', parse_dates=['Datetime'])\n",
    "stock_df3 = pd.read_csv('~/LighthouseLabs-Final/1. Stock_Data/AAPL_data.csv', parse_dates=['Datetime'])\n",
    "stock_df4 = pd.read_csv('~/LighthouseLabs-Final/1. Stock_Data/GOOG_data.csv', parse_dates=['Datetime'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Headline Data:\n",
    "headlines1 = pd.read_csv('~/LighthouseLabs-Final/2. FinViz_Headline_Data/NFLX_data_2020-09-28-22-15.csv', index_col=1, parse_dates=[['date','time']])\n",
    "headlines2 = pd.read_csv('~/LighthouseLabs-Final/2. FinViz_Headline_Data/NFLX_data_2020-09-29-21-32.csv', index_col=1, parse_dates=[['date','time']])\n",
    "#headlines3 = pd.read_csv('~/LighthouseLabs-Final/2. FinViz_Headline_Data/TSLA_data_2020-09-29-21-32.csv', index_col=1, parse_dates=[['date','time']])\n",
    "frames = [headlines1, headlines2]\n",
    "headlines_df1 = pd.concat(frames)\n",
    "headlines_df1.drop_duplicates(subset='headline',keep='first',inplace=True)\n",
    "headlines_df1.to_csv('~/LighthouseLabs-Final/NFLX_2020-09-23_2020-09-29.csv')\n",
    "\n",
    "# headlines1 = pd.read_csv('~/LighthouseLabs-Final/2. FinViz_Headline_Data/NKE_data_2020-09-28-22-15.csv', index_col=1, parse_dates=[['date','time']])\n",
    "# headlines2 = pd.read_csv('~/LighthouseLabs-Final/2. FinViz_Headline_Data/NKE_data_2020-09-29-21-32.csv', index_col=1, parse_dates=[['date','time']])\n",
    "# # headlines3 = pd.read_csv('~/LighthouseLabs-Final/2. FinViz_Headline_Data/GS_data_2020-09-29-21-32.csv', index_col=1, parse_dates=[['date','time']])\n",
    "# frames = [headlines1, headlines2]\n",
    "# headlines_df2 = pd.concat(frames)\n",
    "# headlines_df2.drop_duplicates(subset='headline',keep='first',inplace=True)\n",
    "# # headlines_df2.to_csv('~/LighthouseLabs-Final/NKE_2020-09-23_2020-09-29.csv')\n",
    "\n",
    "# headlines1 = pd.read_csv('~/LighthouseLabs-Final/2. FinViz_Headline_Data/NVDA_data_2020-09-25-23-28.csv', index_col=1, parse_dates=[['date','time']])\n",
    "# headlines2 = pd.read_csv('~/LighthouseLabs-Final/2. FinViz_Headline_Data/NVDA_data_2020-09-27-23-41.csv', index_col=1, parse_dates=[['date','time']])\n",
    "# headlines3 = pd.read_csv('~/LighthouseLabs-Final/2. FinViz_Headline_Data/NVDA_data_2020-09-29-21-32.csv', index_col=1, parse_dates=[['date','time']])\n",
    "# frames = [headlines1, headlines2,headlines3]\n",
    "# headlines_df3 = pd.concat(frames)\n",
    "# headlines_df3.drop_duplicates(subset='headline',keep='first',inplace=True)\n",
    "# # headlines_df3.to_csv('~/LighthouseLabs-Final/NVDA_2020-09-23_2020-09-29.csv')\n",
    "\n",
    "# headlines1 = pd.read_csv('~/LighthouseLabs-Final/2. FinViz_Headline_Data/PFE_data_2020-09-28-22-15.csv', index_col=1, parse_dates=[['date','time']])\n",
    "# headlines2 = pd.read_csv('~/LighthouseLabs-Final/2. FinViz_Headline_Data/PFE_data_2020-09-29-21-32.csv', index_col=1, parse_dates=[['date','time']])\n",
    "# # headlines3 = pd.read_csv('~/LighthouseLabs-Final/2. FinViz_Headline_Data/MSFT_data_2020-09-29-21-32.csv', index_col=1, parse_dates=[['date','time']])\n",
    "# frames = [headlines1, headlines2]\n",
    "# headlines_df4 = pd.concat(frames)\n",
    "# headlines_df4.drop_duplicates(subset='headline',keep='first',inplace=True)\n",
    "# # headlines_df4.to_csv('~/LighthouseLabs-Final/PFE_2020-09-23_2020-09-29.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: &#39;C:\\\\Users\\\\keato/LighthouseLabs-Final/3. Twitter_Data/MSFT_2020-09-23.csv&#39;",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m&lt;ipython-input-313-283e4b35c1e6&gt;\u001b[0m in \u001b[0;36m&lt;module&gt;\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mtwitter_df1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m&#39;~/LighthouseLabs-Final/NVDA_2020-09-23_2020-09-29.csv&#39;\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---&gt; 12\u001b[1;33m \u001b[0mtwitter1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m&#39;~/LighthouseLabs-Final/3. Twitter_Data/MSFT_2020-09-23.csv&#39;\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparse_dates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m&#39;timestamp&#39;\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0mtwitter2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m&#39;~/LighthouseLabs-Final/3. Twitter_Data/MSFT_2020-09-24.csv&#39;\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparse_dates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m&#39;timestamp&#39;\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mtwitter3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m&#39;~/LighthouseLabs-Final/3. Twitter_Data/MSFT_2020-09-25.csv&#39;\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparse_dates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m&#39;timestamp&#39;\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--&gt; 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--&gt; 452\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    934\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m&quot;has_index_names&quot;\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m&quot;has_index_names&quot;\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--&gt; 936\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    937\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    938\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1166\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m&quot;c&quot;\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1167\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m&quot;c&quot;\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-&gt; 1168\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1169\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1170\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m&quot;python&quot;\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1996\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m&quot;usecols&quot;\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1997\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-&gt; 1998\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1999\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2000\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: &#39;C:\\\\Users\\\\keato/LighthouseLabs-Final/3. Twitter_Data/MSFT_2020-09-23.csv&#39;"
     ]
    }
   ],
   "source": [
    "# 3. Twitter Data:\n",
    "twitter1 = pd.read_csv('~/LighthouseLabs-Final/3. Twitter_Data/NVDA_2020-09-23_2020-09-29.csv', parse_dates=['timestamp'])\n",
    "# twitter2 = pd.read_csv('~/LighthouseLabs-Final/3. Twitter_Data/NVDA_2020-09-24.csv', parse_dates=['timestamp'])\n",
    "# twitter3 = pd.read_csv('~/LighthouseLabs-Final/3. Twitter_Data/GS_2020-09-25.csv', parse_dates=['timestamp'])\n",
    "# twitter4 = pd.read_csv('~/LighthouseLabs-Final/3. Twitter_Data/GS_2020-09-28.csv', parse_dates=['timestamp'])\n",
    "twitter5 = pd.read_csv('~/LighthouseLabs-Final/3. Twitter_Data/NVDA_2020-09-29.csv', parse_dates=['timestamp'])\n",
    "frames = [twitter1,twitter5]\n",
    "twitter_df1 = pd.concat(frames)\n",
    "twitter_df1.drop_duplicates(subset='tweet_text',keep='first', inplace=True)\n",
    "twitter_df1.to_csv('~/LighthouseLabs-Final/_2020-09-23_2020-09-29.csv')\n",
    "\n",
    "twitter1 = pd.read_csv('~/LighthouseLabs-Final/3. Twitter_Data/MSFT_2020-09-23.csv', parse_dates=['timestamp'])\n",
    "twitter2 = pd.read_csv('~/LighthouseLabs-Final/3. Twitter_Data/MSFT_2020-09-24.csv', parse_dates=['timestamp'])\n",
    "twitter3 = pd.read_csv('~/LighthouseLabs-Final/3. Twitter_Data/MSFT_2020-09-25.csv', parse_dates=['timestamp'])\n",
    "frames = [twitter1, twitter2, twitter3]\n",
    "twitter_df2 = pd.concat(frames)\n",
    "twitter_df2.drop_duplicates(subset='tweet_text',keep='first', inplace=True)\n",
    "# twitter_df2.to_csv('~/LighthouseLabs-Final/NKE_2020-09-23_2020-09-29.csv')\n",
    "\n",
    "twitter1 = pd.read_csv('~/LighthouseLabs-Final/3. Twitter_Data/MSFT_2020-09-23.csv', parse_dates=['timestamp'])\n",
    "twitter2 = pd.read_csv('~/LighthouseLabs-Final/3. Twitter_Data/MSFT_2020-09-24.csv', parse_dates=['timestamp'])\n",
    "twitter3 = pd.read_csv('~/LighthouseLabs-Final/3. Twitter_Data/MSFT_2020-09-25.csv', parse_dates=['timestamp'])\n",
    "frames = [twitter1, twitter2, twitter3]\n",
    "twitter_df3 = pd.concat(frames)\n",
    "twitter_df3.drop_duplicates(subset='tweet_text',keep='first', inplace=True)\n",
    "# twitter_df3.to_csv('~/LighthouseLabs-Final/NVDA_2020-09-23_2020-09-29.csv')\n",
    "\n",
    "twitter1 = pd.read_csv('~/LighthouseLabs-Final/3. Twitter_Data/MSFT_2020-09-23.csv', parse_dates=['timestamp'])\n",
    "twitter2 = pd.read_csv('~/LighthouseLabs-Final/3. Twitter_Data/MSFT_2020-09-24.csv', parse_dates=['timestamp'])\n",
    "twitter3 = pd.read_csv('~/LighthouseLabs-Final/3. Twitter_Data/MSFT_2020-09-25.csv', parse_dates=['timestamp'])\n",
    "frames = [twitter1, twitter2, twitter3]\n",
    "twitter_df4 = pd.concat(frames)\n",
    "twitter_df4.drop_duplicates(subset='tweet_text',keep='first', inplace=True)\n",
    "# twitter_df4.to_csv('~/LighthouseLabs-Final/PFE_2020-09-23_2020-09-29.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_df(stock_df, headline_df, twitter_df):\n",
    "    stock_df['Datetime'] = stock_df['Datetime'].dt.tz_convert('America/Montreal').dt.tz_localize(None)\n",
    "    #2. Headline Final Merge:\n",
    "    headlines_final = preprocess_headlines(headline_df)\n",
    "    with_headlines_df = stock_df.merge(headlines_final, left_on='Datetime', right_on='date_time').drop('date_time',axis=1)\n",
    "    with_headlines_df['t+1'] = with_headlines_df['Adj Close'].shift(-1)\n",
    "    #3. Twitter Final Merge:\n",
    "    final_twitter = preprocess_posts(twitter_df)\n",
    "    with_twitter_df = stock_df.merge(final_twitter, left_on='Datetime', right_on='timestamp').drop('timestamp',axis=1)\n",
    "    with_twitter_df['t+1'] = with_twitter_df['Adj Close'].shift(-1)\n",
    "    #4. Full Merge:\n",
    "    full_df = with_twitter_df.merge(headlines_final, left_on='Datetime', right_on='date_time').drop('date_time',axis=1)\n",
    "    full_df['t+1'] = full_df['Adj Close'].shift(-1)\n",
    "\n",
    "    return with_headlines_df,with_twitter_df,full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsla_headlines_df, tsla_twitter_df, tsla_full_df = cleaning_df(stock_df1, headlines_df1, twitter_df1)\n",
    "amzn_headlines_df, amzn_twitter_df, amzn_full_df = cleaning_df(stock_df2, headlines_df2, twitter_df2)\n",
    "aapl_headlines_df, aapl_twitter_df, aapl_full_df = cleaning_df(stock_df3, headlines_df3, twitter_df3)\n",
    "goog_headlines_df, goog_twitter_df, goog_full_df = cleaning_df(stock_df4, headlines_df4, twitter_df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_df1['t+1'] = stock_df1['Adj Close'].shift(-1)\n",
    "stock_df2['t+1'] = stock_df2['Adj Close'].shift(-1)\n",
    "stock_df3['t+1'] = stock_df3['Adj Close'].shift(-1)\n",
    "stock_df4['t+1'] = stock_df4['Adj Close'].shift(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_frames = [stock_df1, stock_df2, stock_df3, stock_df4]\n",
    "full_stocks = pd.concat(stock_frames)\n",
    "\n",
    "headline_frames = [tsla_headlines_df, amzn_headlines_df, aapl_headlines_df, goog_headlines_df]\n",
    "full_headlines = pd.concat(headline_frames)\n",
    "\n",
    "twitter_frames = [tsla_twitter_df,amzn_twitter_df,aapl_twitter_df,goog_twitter_df]\n",
    "full_twitter = pd.concat(twitter_frames)\n",
    "\n",
    "full_frames = [tsla_full_df,amzn_full_df,aapl_full_df,goog_full_df]\n",
    "full_final = pd.concat(full_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_model_full2(dataframe):\n",
    "    i = len(dataframe['t+1'])-4\n",
    "    y_train, y_test = dataframe['t+1'][:i], dataframe['t+1'][i:-1]\n",
    "    X_train, X_test = dataframe[['Adj Close','Scaled Volume','compound_y','compound_x','SMA(3)','change in sentiment headlines','change in sentiment headlines (t-1)','change in sentiment twitter','change in sentiment twitter (t-1)']][:i], dataframe[['Adj Close','Scaled Volume','compound_y','compound_x','SMA(3)','change in sentiment headlines','change in sentiment headlines (t-1)','change in sentiment twitter','change in sentiment twitter (t-1)']][i:-1]\n",
    "\n",
    "    lm = LinearRegression()\n",
    "    lm.fit(X_train,y_train)\n",
    "    predictions = lm.predict(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test,predictions))\n",
    "    r2_sco = r2_score(y_test,predictions)\n",
    "    #print('LR Root Mean Squared Error: ',rmse)\n",
    "    #print('LR R2 Score: ', r2_sco,'\\n')\n",
    "\n",
    "    reg = SGDRegressor(random_state=42)\n",
    "    reg.fit(X_train, y_train)\n",
    "    predictions2 = reg.predict(X_test)\n",
    "    rmse2 = np.sqrt(mean_squared_error(y_test,predictions2))\n",
    "    r2_sco2 = r2_score(y_test,predictions2)\n",
    "    #print('SGD Root Mean Squared Error: ',rmse2)\n",
    "    #print('SGD R2 Score: ', r2_sco2,'\\n')\n",
    "\n",
    "    xg_reg = xgb.XGBRegressor(booster='gblinear', learning_rate = 0.03, n_estimators = 10000)\n",
    "    xg_reg.fit(X_train,y_train)\n",
    "    preds3 = xg_reg.predict(X_test)\n",
    "    rmse3 = np.sqrt(mean_squared_error(y_test, preds3))\n",
    "    r2_sco3 = r2_score(y_test,preds3)\n",
    "    #print('XGB Mean Squared Error: ',rmse3)\n",
    "    #print('XGB R2 Score: ', r2_sco3)\n",
    "\n",
    "    rf_regr = RandomForestRegressor(n_estimators=20, max_depth=600, random_state=42)\n",
    "    rf_regr.fit(X_train,y_train)\n",
    "    preds4 = rf_regr.predict(X_test)\n",
    "    rmse4 = np.sqrt(mean_squared_error(y_test, preds4))\n",
    "    r2_sco4 = r2_score(y_test,preds4)\n",
    "    return rmse,r2_sco,rmse2,r2_sco2,rmse3,r2_sco3,rmse4,r2_sco4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_stocks.dropna(inplace=True)\n",
    "lm_baseline_rmse,lm_baseline_r2,sgd_baseline_rmse,sgd_baseline_r2 = linear_modeling_no_sentiment(full_stocks)\n",
    "\n",
    "full_headlines.dropna(inplace=True)\n",
    "lm_headlines_rmse,lm_headlines_r2,sgd_headlines_rmse,sgd_headlines_r2,xgb_headlines_rmse,xgb_headlines_r2  = linear_modeling_headlines(full_headlines)\n",
    "\n",
    "full_twitter.dropna(inplace=True)\n",
    "lm_twitter_rmse,lm_twitter_r2,sgd_twitter_rmse,sgd_twitter_r2,xgb_twitter_rmse,xgb_twitter_r2=linear_model_twitter(full_twitter)\n",
    "\n",
    "full_final.dropna(inplace=True)\n",
    "lm_all_rmse,lm_all_r2,sgd_all_rmse,sgd_all_r2,xgb_all_rmse,xgb_all_r2,rf_all_rmse,rf_all_r2=multi_model_full2(full_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = {\n",
    "    'Linear RMSE - Baseline':lm_baseline_rmse, 'Linear R2 - Baseline':lm_baseline_r2, 'SGD RMSE - Baseline':sgd_baseline_rmse, 'SGD R2 - Baseline':sgd_baseline_r2,\n",
    "    'Linear RMSE - Only Headlines': lm_headlines_rmse, 'Linear R2 - Only Headlines':lm_headlines_r2, 'SGD RMSE - Only Headlines':sgd_headlines_rmse, 'SGD R2 - Only Headlines':sgd_headlines_r2, 'XGB RMSE - Only Headlines':xgb_headlines_rmse, 'XGB R2 - Only Headlines':xgb_headlines_r2,\n",
    "    'Linear RMSE - Only Twitter':lm_twitter_rmse, 'Linear R2 - Only Twitter':lm_twitter_r2, 'SGD RMSE - Only Twitter':sgd_twitter_rmse, 'SGD R2 - Only Twitter':sgd_twitter_r2, 'XGB RMSE - Only Twitter':xgb_twitter_rmse, 'XGB R2 - Only Twitter':xgb_twitter_r2,\n",
    "    'Linear RMSE - All':lm_all_rmse, 'Linear R2 - All':lm_all_r2, 'SGD RMSE - All':sgd_all_rmse, 'SGD R2 - All':sgd_all_r2, 'XGB RMSE - All':xgb_all_rmse, 'XGB R2 - All':xgb_all_r2, 'RF RMSE - All':rf_all_rmse,'RF R2 - All':rf_all_r2}\n",
    "    #7. Convert to DataFrame:\n",
    "result_df = pd.DataFrame.from_dict(result_dict, orient='index', columns=['Values'])\n",
    "    #result_df.to_csv('~/LighthouseLabs-Final/Report_Analysis/AAPL_complete_analysis.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                    Values\nLinear RMSE - Baseline        1.394576e+00\nLinear R2 - Baseline         -4.620149e+00\nSGD RMSE - Baseline           1.061594e+15\nSGD R2 - Baseline            -3.256723e+30\nLinear RMSE - Only Headlines  1.356657e+00\nLinear R2 - Only Headlines   -4.318677e+00\nSGD RMSE - Only Headlines     3.003464e+15\nSGD R2 - Only Headlines      -2.606805e+31\nXGB RMSE - Only Headlines     1.346430e+00\nXGB R2 - Only Headlines      -4.238790e+00\nLinear RMSE - Only Twitter    1.486534e+00\nLinear R2 - Only Twitter     -5.385774e+00\nSGD RMSE - Only Twitter       3.208230e+15\nSGD R2 - Only Twitter        -2.974367e+31\nXGB RMSE - Only Twitter       8.624445e-01\nXGB R2 - Only Twitter        -1.149443e+00\nLinear RMSE - All             1.559939e+00\nLinear R2 - All              -6.031997e+00\nSGD RMSE - All                1.141385e+15\nSGD R2 - All                 -3.764681e+30\nXGB RMSE - All                6.164611e-01\nXGB R2 - All                 -9.818391e-02\nRF RMSE - All                 6.627884e+00\nRF R2 - All                  -1.259444e+02",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Values</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Linear RMSE - Baseline</th>\n      <td>1.394576e+00</td>\n    </tr>\n    <tr>\n      <th>Linear R2 - Baseline</th>\n      <td>-4.620149e+00</td>\n    </tr>\n    <tr>\n      <th>SGD RMSE - Baseline</th>\n      <td>1.061594e+15</td>\n    </tr>\n    <tr>\n      <th>SGD R2 - Baseline</th>\n      <td>-3.256723e+30</td>\n    </tr>\n    <tr>\n      <th>Linear RMSE - Only Headlines</th>\n      <td>1.356657e+00</td>\n    </tr>\n    <tr>\n      <th>Linear R2 - Only Headlines</th>\n      <td>-4.318677e+00</td>\n    </tr>\n    <tr>\n      <th>SGD RMSE - Only Headlines</th>\n      <td>3.003464e+15</td>\n    </tr>\n    <tr>\n      <th>SGD R2 - Only Headlines</th>\n      <td>-2.606805e+31</td>\n    </tr>\n    <tr>\n      <th>XGB RMSE - Only Headlines</th>\n      <td>1.346430e+00</td>\n    </tr>\n    <tr>\n      <th>XGB R2 - Only Headlines</th>\n      <td>-4.238790e+00</td>\n    </tr>\n    <tr>\n      <th>Linear RMSE - Only Twitter</th>\n      <td>1.486534e+00</td>\n    </tr>\n    <tr>\n      <th>Linear R2 - Only Twitter</th>\n      <td>-5.385774e+00</td>\n    </tr>\n    <tr>\n      <th>SGD RMSE - Only Twitter</th>\n      <td>3.208230e+15</td>\n    </tr>\n    <tr>\n      <th>SGD R2 - Only Twitter</th>\n      <td>-2.974367e+31</td>\n    </tr>\n    <tr>\n      <th>XGB RMSE - Only Twitter</th>\n      <td>8.624445e-01</td>\n    </tr>\n    <tr>\n      <th>XGB R2 - Only Twitter</th>\n      <td>-1.149443e+00</td>\n    </tr>\n    <tr>\n      <th>Linear RMSE - All</th>\n      <td>1.559939e+00</td>\n    </tr>\n    <tr>\n      <th>Linear R2 - All</th>\n      <td>-6.031997e+00</td>\n    </tr>\n    <tr>\n      <th>SGD RMSE - All</th>\n      <td>1.141385e+15</td>\n    </tr>\n    <tr>\n      <th>SGD R2 - All</th>\n      <td>-3.764681e+30</td>\n    </tr>\n    <tr>\n      <th>XGB RMSE - All</th>\n      <td>6.164611e-01</td>\n    </tr>\n    <tr>\n      <th>XGB R2 - All</th>\n      <td>-9.818391e-02</td>\n    </tr>\n    <tr>\n      <th>RF RMSE - All</th>\n      <td>6.627884e+00</td>\n    </tr>\n    <tr>\n      <th>RF R2 - All</th>\n      <td>-1.259444e+02</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 184
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{&#39;max_depth&#39;: 600, &#39;n_estimators&#39;: 20}"
     },
     "metadata": {},
     "execution_count": 150
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "rf_regr=RandomForestRegressor(criterion='mse')\n",
    "estimators = [18,20,22]\n",
    "max_depths = [200, 400, 500, 600, 700]\n",
    "params = [{'n_estimators': estimators, 'max_depth':max_depths}]\n",
    "clf = GridSearchCV(estimator=rf_regr, param_grid=params, n_jobs =-1)\n",
    "\n",
    "i = len(full_final['t+1'])-4\n",
    "y_train, y_test = full_final['t+1'][:i], full_final['t+1'][i:-1]\n",
    "X_train, X_test = full_final[['Adj Close','Scaled Volume','compound_y','compound_x','SMA(3)','change in sentiment headlines','change in sentiment headlines (t-1)','change in sentiment twitter','change in sentiment twitter (t-1)']][:i], full_final[['Adj Close','Scaled Volume','compound_y','compound_x','SMA(3)','change in sentiment headlines','change in sentiment headlines (t-1)','change in sentiment twitter','change in sentiment twitter (t-1)']][i:-1]\n",
    "\n",
    "best_model = clf.fit(X_train,y_train)\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Fitting 2 folds for each of 50 candidates, totalling 100 fits\n[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n[Parallel(n_jobs=5)]: Done  70 tasks      | elapsed:    3.7s\n[Parallel(n_jobs=5)]: Done 100 out of 100 | elapsed:    7.3s finished\n0.999938915001783\n{&#39;booster&#39;: &#39;gblinear&#39;, &#39;learning_rate&#39;: 1, &#39;n_estimators&#39;: 20000}\n"
    }
   ],
   "source": [
    "xgb1 = xgb.XGBRegressor()\n",
    "parameters = {'booster': ['gblinear'],\n",
    "              'learning_rate': [.03, 0.05, .07,.1,1], #so called `eta` value\n",
    "            #   'colsample_bytree': [0.2,0.4, 0.5, 0.7,1],\n",
    "              'n_estimators': [5, 10, 20, 50, 100, 1000, 5000, 10000, 20000, 40000]}\n",
    "\n",
    "xgb_grid = GridSearchCV(xgb1,\n",
    "                        parameters,\n",
    "                        cv = 2,\n",
    "                        n_jobs = 5,\n",
    "                        verbose=True)\n",
    "\n",
    "xgb_grid.fit(X_train,\n",
    "         y_train)\n",
    "\n",
    "print(xgb_grid.best_score_)\n",
    "print(xgb_grid.best_params_)"
   ]
  }
 ]
}