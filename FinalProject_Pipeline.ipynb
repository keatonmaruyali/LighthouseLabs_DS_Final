{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.9 64-bit ('base': conda)",
   "display_name": "Python 3.7.9 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "73a666d948711064f59d854e9125f239b700b6d583285c0d9848266fb8325020"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import re\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "source": [
    "## 1. Import Data:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 2. Defining Functions:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 2.1 Preprocessing Functions:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_change_sentiment(data, col):\n",
    "    change_in_sent = []\n",
    "    change_in_sent.append(data[col][0])\n",
    "    for i in range(1,len(data[col])):\n",
    "        if data[col][i] == 0:\n",
    "            change_in_sent.append(0)\n",
    "        elif data[col][i] < 0 or data[col][i] > 0:\n",
    "            dif = data[col][i] - data[col][(i-1)]\n",
    "            change_in_sent.append(dif)\n",
    "    return change_in_sent\n",
    "\n",
    "def remove_pattern(input_txt, pattern):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for i in r:\n",
    "        input_txt = re.sub(i, '', input_txt)       \n",
    "    return input_txt\n",
    "    \n",
    "def clean_tweets(tweets):\n",
    "    #remove twitter Return handles (RT @xxx:)\n",
    "    tweets = np.vectorize(remove_pattern)(tweets, \"RT @[\\w]*:\") \n",
    "    \n",
    "    #remove twitter handles (@xxx)\n",
    "    tweets = np.vectorize(remove_pattern)(tweets, \"@[\\w]*\")\n",
    "    \n",
    "    #remove URL links (httpxxx)\n",
    "    tweets = np.vectorize(remove_pattern)(tweets, \"https?://[A-Za-z0-9./]*\")\n",
    "    \n",
    "    tweets = np.vectorize(remove_pattern)(tweets, \"b'\")\n",
    "    tweets = np.vectorize(remove_pattern)(tweets, 'b\"')\n",
    "\n",
    "\n",
    "    #remove special characters, numbers, punctuations (except for #)\n",
    "    tweets = np.core.defchararray.replace(tweets, \"[^a-zA-Z]\", \" \")\n",
    "    \n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_news(dataframe, datetime_column_name):\n",
    "\n",
    "    day22, day23, day24, day25, day26, day27, day28, day29, day30, day31, day32, day33, day34 = [],[],[],[],[],[],[],[],[],[],[],[],[]\n",
    "\n",
    "    for i in range(len(dataframe[datetime_column_name])):\n",
    "        #if dataframe[datetime_column_name][i].day == 21 and dataframe[datetime_column_name][i].hour > 17:day22.append(i)\n",
    "        if dataframe[datetime_column_name][i].day == 22 and (dataframe[datetime_column_name][i].hour <= 15 and dataframe[datetime_column_name][i].hour >= 9):\n",
    "            day22.append(i)\n",
    "        #if dataframe[datetime_column_name][i].day == 22 and dataframe[datetime_column_name][i].hour > 17:day23.append(i)\n",
    "        elif dataframe[datetime_column_name][i].day == 23 and (dataframe[datetime_column_name][i].hour <= 15 and dataframe[datetime_column_name][i].hour >= 9):\n",
    "            day23.append(i)\n",
    "        #if dataframe[datetime_column_name][i].day == 23 and dataframe[datetime_column_name][i].hour > 17:day24.append(i)\n",
    "        elif dataframe[datetime_column_name][i].day == 24 and (dataframe[datetime_column_name][i].hour <= 15 and dataframe[datetime_column_name][i].hour >= 9):\n",
    "            day24.append(i)       \n",
    "        #if dataframe[datetime_column_name][i].day == 24 and dataframe[datetime_column_name][i].hour > 17:day25.append(i)\n",
    "        elif dataframe[datetime_column_name][i].day == 25 and (dataframe[datetime_column_name][i].hour <= 15 and dataframe[datetime_column_name][i].hour >= 9):\n",
    "            day25.append(i)\n",
    "        #if dataframe[datetime_column_name][i].day == 25 and dataframe[datetime_column_name][i].hour > 17:day26.append(i)\n",
    "        elif dataframe[datetime_column_name][i].day == 26 and (dataframe[datetime_column_name][i].hour <= 15 and dataframe[datetime_column_name][i].hour >= 9):\n",
    "            day26.append(i)\n",
    "        #if dataframe[datetime_column_name][i].day == 26 and dataframe[datetime_column_name][i].hour > 17:day27.append(i)\n",
    "        elif dataframe[datetime_column_name][i].day == 27 and (dataframe[datetime_column_name][i].hour <= 15 and dataframe[datetime_column_name][i].hour >= 9):\n",
    "            day27.append(i)\n",
    "        #if dataframe[datetime_column_name][i].day == 27 and dataframe[datetime_column_name][i].hour > 17:day28.append(i)\n",
    "        elif dataframe[datetime_column_name][i].day == 28 and (dataframe[datetime_column_name][i].hour <= 15 and dataframe[datetime_column_name][i].hour >= 9):\n",
    "            day28.append(i)\n",
    "        #if dataframe[datetime_column_name][i].day == 28 and dataframe[datetime_column_name][i].hour > 17:day29.append(i)\n",
    "        elif dataframe[datetime_column_name][i].day == 29 and (dataframe[datetime_column_name][i].hour <= 15 and dataframe[datetime_column_name][i].hour >= 9):\n",
    "            day29.append(i)\n",
    "        #if dataframe[datetime_column_name][i].day == 29 and dataframe[datetime_column_name][i].hour > 17:day30.append(i)\n",
    "        elif dataframe[datetime_column_name][i].day == 30 and (dataframe[datetime_column_name][i].hour <= 15 and dataframe[datetime_column_name][i].hour >= 9):\n",
    "            day30.append(i)\n",
    "        elif dataframe[datetime_column_name][i].day == 1 and (dataframe[datetime_column_name][i].hour <= 15 and dataframe[datetime_column_name][i].hour >= 9):\n",
    "            day31.append(i)\n",
    "        elif dataframe[datetime_column_name][i].day == 2 and (dataframe[datetime_column_name][i].hour <= 15 and dataframe[datetime_column_name][i].hour >= 9):\n",
    "            day32.append(i)\n",
    "        elif dataframe[datetime_column_name][i].day == 3 and (dataframe[datetime_column_name][i].hour <= 15 and dataframe[datetime_column_name][i].hour >= 9):\n",
    "            day33.append(i)\n",
    "        elif dataframe[datetime_column_name][i].day == 4 and (dataframe[datetime_column_name][i].hour <= 15 and dataframe[datetime_column_name][i].hour >= 9):\n",
    "            day34.append(i)\n",
    "\n",
    "        else:\n",
    "            pass\n",
    "    news_d22, news_d23,news_d24,news_d25,news_d26,news_d27,news_d28,news_d29,news_d30,news_d31,news_d32,news_d33,news_d34 = dataframe.iloc[day22],dataframe.iloc[day23],dataframe.iloc[day24],dataframe.iloc[day25], dataframe.iloc[day26], dataframe.iloc[day27],dataframe.iloc[day28],dataframe.iloc[day29],dataframe.iloc[day30],dataframe.iloc[day31], dataframe.iloc[day32],dataframe.iloc[day33],dataframe.iloc[day34]\n",
    "    return news_d22, news_d23,news_d24,news_d25,news_d26,news_d27,news_d28,news_d29,news_d30,news_d31,news_d32,news_d33,news_d34\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_headlines(data):\n",
    "    data.drop_duplicates(subset='headline',keep=False, inplace=True)\n",
    "    data.drop('ticker', axis=1, inplace=True)\n",
    "    data.set_index('date_time', inplace=True)\n",
    "    data_30m = data.resample('30min').median().ffill().reset_index()\n",
    "    change_in_sent=calc_change_sentiment(data_30m, 'compound')\n",
    "    data_30m['change in sentiment headlines'] = change_in_sent\n",
    "\n",
    "    news_d22, news_d23,news_d24,news_d25,news_d26,news_d27,news_d28,news_d29,news_d30,news_d31,news_d32,news_d33,news_d34 = classify_news(data_30m, 'date_time')\n",
    "\n",
    "    news_d23_red,news_d24_red,news_d25_red,news_d26_red,news_d27_red,news_d28_red,news_d29_red,news_d30_red,news_d31_red,news_d32_red,news_d33_red,news_d34_red = news_d23.iloc[4:],news_d24.iloc[1:],news_d25.iloc[1:],news_d26.iloc[1:],news_d27.iloc[1:],news_d28.iloc[1:],news_d29.iloc[1:],news_d30.iloc[1:],news_d31.iloc[1:],news_d32.iloc[1:],news_d33.iloc[1:],news_d34.iloc[1:]\n",
    "\n",
    "    frames_news = [news_d23_red,news_d24_red, news_d25_red, news_d28_red]\n",
    "    netflix_headlines_30m_d23_24_25 = pd.concat(frames_news)\n",
    "    return netflix_headlines_30m_d23_24_25\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_posts(dataframe):\n",
    "    vader = SentimentIntensityAnalyzer()\n",
    "    dataframe['tweet_text'] = clean_tweets(dataframe['tweet_text'])\n",
    "    scores = dataframe['tweet_text'].apply(vader.polarity_scores).tolist()\n",
    "    scores_df = pd.DataFrame(scores)\n",
    "\n",
    "    df = dataframe.join(scores_df, rsuffix='_right')\n",
    "    df = df[['timestamp','tweet_text','followers_count','neg','neu','pos','compound']]\n",
    "    df['timestamp'] = df['timestamp'].dt.tz_localize('UTC').dt.tz_convert('America/Montreal').dt.tz_localize(None)\n",
    "    df['scaled_followers_count'] =(df['followers_count']/df['followers_count'].max()) + 1\n",
    "    df['adj compound'] = df['compound']*df['scaled_followers_count']\n",
    "    df.set_index('timestamp', inplace=True)\n",
    "\n",
    "    twitter_df_hourly = df.resample('30min').median().ffill().reset_index()\n",
    "    change_in_sent = calc_change_sentiment(twitter_df_hourly, 'adj compound')\n",
    "    twitter_df_hourly['change in sentiment twitter'] = change_in_sent\n",
    "\n",
    "    tw_news_d22,tw_news_d23,tw_news_d24,tw_news_d25,tw_news_d26,tw_news_d27,tw_news_d28,tw_news_d29,tw_news_d30,tw_news_d31,tw_news_d32,tw_news_d33,tw_news_d34 = classify_news(twitter_df_hourly, 'timestamp')\n",
    "\n",
    "    tw_news_d23_30m,tw_news_d24_30m,tw_news_d25_30m, tw_news_d26_30m,tw_news_d27_30m,tw_news_d28_30m,tw_news_d29_30m,tw_news_d30_30m,tw_news_d31_30m,tw_news_d32_30m,tw_news_d33_30m,tw_news_d34_30m = tw_news_d23.iloc[4:],tw_news_d24.iloc[1:],tw_news_d25.iloc[1:],tw_news_d26.iloc[1:],tw_news_d27.iloc[1:],tw_news_d28.iloc[1:],tw_news_d29.iloc[1:],tw_news_d30.iloc[1:],tw_news_d31.iloc[1:],tw_news_d32.iloc[1:],tw_news_d33.iloc[1:],tw_news_d34.iloc[1:]\n",
    "\n",
    "    frames = [tw_news_d23_30m,tw_news_d24_30m,tw_news_d25_30m, tw_news_d28_30m,tw_news_d29_30m,tw_news_d30_30m,tw_news_d31_30m,tw_news_d32_30m,tw_news_d33_30m,tw_news_d34_30m]\n",
    "    tw_news_30min = pd.concat(frames)\n",
    "    return tw_news_30min"
   ]
  },
  {
   "source": [
    "### 2.2 Modeling Functions:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_model(data):\n",
    "    data['Datetime'] = data['Datetime'].dt.tz_convert('America/Montreal').dt.tz_localize(None)\n",
    "    # data_SMA = data['Adj Close'].rolling(window=3).mean().shift(1)\n",
    "    # data['SMA(3)'] = data_SMA\n",
    "    pred = data['SMA(3)'][3:]\n",
    "    actu = data['Adj Close'][3:]\n",
    "    rmse = np.sqrt(mean_squared_error(actu,pred))\n",
    "    r2_sco = r2_score(actu,pred)\n",
    "    # print('Root Mean Squared Error: ',rmse)\n",
    "    # print('R2 Score: ', r2_sco)\n",
    "    return rmse, r2_sco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_modeling_no_sentiment(dataframe):\n",
    "    i = len(dataframe['t+1'])-4\n",
    "    y_train, y_test = dataframe['t+1'][3:i], dataframe['t+1'][i:-1]\n",
    "    X_train, X_test = dataframe[['Adj Close','Scaled Volume','SMA(3)']][3:i], dataframe[['Adj Close','Scaled Volume','SMA(3)']][i:-1]\n",
    "\n",
    "    lm = LinearRegression()\n",
    "    lm.fit(X_train,y_train)\n",
    "    predictions = lm.predict(X_test)\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(y_test,predictions))\n",
    "    r2_sco = r2_score(y_test,predictions)\n",
    "    # print('LR Root Mean Squared Error: ',rmse)\n",
    "    # print('LR R2 Score: ', r2_sco, '\\n')\n",
    "    \n",
    "    reg = SGDRegressor(random_state=42)\n",
    "    reg.fit(X_train, y_train)\n",
    "    predictions2 = reg.predict(X_test)\n",
    "    rmse2 = np.sqrt(mean_squared_error(y_test,predictions2))\n",
    "    r2_sco2 = r2_score(y_test,predictions2)\n",
    "    # print('SGD Root Mean Squared Error: ',rmse2)\n",
    "    # print('SGD R2 Score: ', r2_sco2)\n",
    "    return rmse,r2_sco,rmse2,r2_sco2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_modeling_headlines(dataframe):\n",
    "    i = len(dataframe['t+1'])-4\n",
    "    y_train, y_test = dataframe['t+1'][:i], dataframe['t+1'][i:-1]\n",
    "    X_train, X_test = dataframe[['Adj Close','Scaled Volume','compound','SMA(3)']][:i], dataframe[['Adj Close','Scaled Volume','compound','SMA(3)']][i:-1]\n",
    "\n",
    "    lm = LinearRegression()\n",
    "    lm.fit(X_train,y_train)\n",
    "    predictions = lm.predict(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test,predictions))\n",
    "    r2_sco = r2_score(y_test,predictions)\n",
    "    # print('LR Root Mean Squared Error: ',rmse)\n",
    "    # print('LR R2 Score: ', r2_sco,'\\n')\n",
    "    \n",
    "    reg = SGDRegressor(random_state=42)\n",
    "    reg.fit(X_train, y_train)\n",
    "    predictions2 = reg.predict(X_test)\n",
    "    rmse2 = np.sqrt(mean_squared_error(y_test,predictions2))\n",
    "    r2_sco2 = r2_score(y_test,predictions2)\n",
    "    # print('SGD Root Mean Squared Error: ',rmse2)\n",
    "    # print('SGD R2 Score: ', r2_sco2)\n",
    "    return rmse,r2_sco,rmse2,r2_sco2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_model_twitter(dataframe):\n",
    "    i = len(dataframe['t+1'])-4\n",
    "    y_train, y_test = dataframe['t+1'][:i], dataframe['t+1'][i:-1]\n",
    "    X_train, X_test = dataframe[['Adj Close','Scaled Volume','compound','SMA(3)']][:i], dataframe[['Adj Close','Scaled Volume','compound','SMA(3)']][i:-1]\n",
    "\n",
    "    lm = LinearRegression()\n",
    "    lm.fit(X_train,y_train)\n",
    "    predictions = lm.predict(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test,predictions))\n",
    "    r2_sco = r2_score(y_test,predictions)\n",
    "    # print('LR Root Mean Squared Error: ',rmse)\n",
    "    # print('LR R2 Score: ', r2_sco,'\\n')\n",
    "\n",
    "    reg = SGDRegressor(random_state=42)\n",
    "    reg.fit(X_train, y_train)\n",
    "    predictions2 = reg.predict(X_test)\n",
    "    rmse2 = np.sqrt(mean_squared_error(y_test,predictions2))\n",
    "    r2_sco2 = r2_score(y_test,predictions2)\n",
    "    # print('SGD Root Mean Squared Error: ',rmse2)\n",
    "    # print('SGD R2 Score: ', r2_sco2)\n",
    "    return rmse,r2_sco,rmse2,r2_sco2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_model_full(dataframe):\n",
    "    i = len(dataframe['t+1'])-4\n",
    "    y_train, y_test = dataframe['t+1'][:i], dataframe['t+1'][i:-1]\n",
    "    X_train, X_test = dataframe[['Adj Close','Scaled Volume','compound_y','compound_x','SMA(3)']][:i], dataframe[['Adj Close','Scaled Volume','compound_y','compound_x','SMA(3)']][i:-1]\n",
    "\n",
    "    lm = LinearRegression()\n",
    "    lm.fit(X_train,y_train)\n",
    "    predictions = lm.predict(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test,predictions))\n",
    "    r2_sco = r2_score(y_test,predictions)\n",
    "    #print('LR Root Mean Squared Error: ',rmse)\n",
    "    #print('LR R2 Score: ', r2_sco,'\\n')\n",
    "\n",
    "    reg = SGDRegressor(random_state=42)\n",
    "    reg.fit(X_train, y_train)\n",
    "    predictions2 = reg.predict(X_test)\n",
    "    rmse2 = np.sqrt(mean_squared_error(y_test,predictions2))\n",
    "    r2_sco2 = r2_score(y_test,predictions2)\n",
    "    #print('SGD Root Mean Squared Error: ',rmse2)\n",
    "    #print('SGD R2 Score: ', r2_sco2,'\\n')\n",
    "\n",
    "    xg_reg = xgb.XGBRegressor(booster='gbtree', colsample_bytree = 0.7, eta = 0.03, max_depth = 3, n_estimators = 10000)\n",
    "    xg_reg.fit(X_train,y_train)\n",
    "    preds3 = xg_reg.predict(X_test)\n",
    "    rmse3 = np.sqrt(mean_squared_error(y_test, preds3))\n",
    "    r2_sco3 = r2_score(y_test,preds3)\n",
    "    #print('XGB Mean Squared Error: ',rmse3)\n",
    "    #print('XGB R2 Score: ', r2_sco3)\n",
    "\n",
    "    rf_regr = RandomForestRegressor(n_estimators=1000, criterion='mse', max_depth=2, random_state=42)\n",
    "    rf_regr.fit(X_train,y_train)\n",
    "    preds4 = rf_regr.predict(X_test)\n",
    "    rmse4 = np.sqrt(mean_squared_error(y_test, preds4))\n",
    "    r2_sco4 = r2_score(y_test,preds4)\n",
    "    return rmse,r2_sco,rmse2,r2_sco2,rmse3,r2_sco3,rmse4,r2_sco4"
   ]
  },
  {
   "source": [
    "## 3. Pipeline:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Historical Stock Data:\n",
    "stock_df = pd.read_csv('~/LighthouseLabs-Final/1. Stock_Data/TSLA_data.csv', parse_dates=['Datetime'])\n",
    "\n",
    "# 2. Headline Data:\n",
    "headlines1 = pd.read_csv('~/LighthouseLabs-Final/2. FinViz_Headline_Data/TSLA_data_2020-09-25-10-14.csv', index_col=1, parse_dates=[['date','time']])\n",
    "headlines2 = pd.read_csv('~/LighthouseLabs-Final/2. FinViz_Headline_Data/TSLA_data_2020-09-28-22-15.csv', index_col=1, parse_dates=[['date','time']])\n",
    "frames = [headlines1, headlines2]\n",
    "headlines_df = pd.concat(frames)\n",
    "headlines_df.drop_duplicates(subset='headline',keep='first',inplace=True)\n",
    "\n",
    "# 3. Twitter Data:\n",
    "twitter_d23 = pd.read_csv('~/LighthouseLabs-Final/3. Twitter_Data/TSLA_2020-09-23.csv', parse_dates=['timestamp'])\n",
    "twitter_d23_1 = pd.read_csv('~/LighthouseLabs-Final/3. Twitter_Data/TSLA_2020-09-23-21-07.csv', parse_dates=['timestamp'])\n",
    "twitter_d24 = pd.read_csv('~/LighthouseLabs-Final/3. Twitter_Data/TSLA_2020-09-24.csv', parse_dates=['timestamp'])\n",
    "#twitter_d24_1 = pd.read_csv('~/LighthouseLabs-Final/3. Twitter_Data/FB_2020-09-24-13-22.csv', parse_dates=['timestamp'])\n",
    "twitter_d25 = pd.read_csv('~/LighthouseLabs-Final/3. Twitter_Data/TSLA_2020-09-25.csv', parse_dates=['timestamp'])\n",
    "twitter_d28 = pd.read_csv('~/LighthouseLabs-Final/3. Twitter_Data/NVDA_2020-09-28.csv', parse_dates=['timestamp'])\n",
    "twitter_d28_2 = pd.read_csv('~/LighthouseLabs-Final/3. Twitter_Data/TSLA_2020-09-28_3.csv', parse_dates=['timestamp'])\n",
    "frames = [twitter_d23,twitter_d23_1, twitter_d24, twitter_d25, twitter_d28,twitter_d28_2]\n",
    "twitter_df = pd.concat(frames)\n",
    "twitter_df.drop_duplicates(subset='tweet_text',keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_models(baseline_df, headline_df, twitter_df):\n",
    "    #1. Baseline:\n",
    "    baseline_rmse, baseline_r2 = baseline_model(baseline_df)\n",
    "    baseline_df2 = baseline_df\n",
    "    baseline_df2['t+1'] = baseline_df2['Adj Close'].shift(-1)\n",
    "    lm_baseline_rmse, lm_baseline_r2, sgd_baseline_rmse, sgd_baseline_r2 = linear_modeling_no_sentiment(baseline_df2)\n",
    "    #2. Headline Final Merge:\n",
    "    headlines_final = preprocess_headlines(headline_df)\n",
    "    with_headlines_df = stock_df.merge(headlines_final, left_on='Datetime', right_on='date_time').drop('date_time',axis=1)\n",
    "    with_headlines_df['t+1'] = with_headlines_df['Adj Close'].shift(-1)\n",
    "    #3. Twitter Final Merge:\n",
    "    final_twitter = preprocess_posts(twitter_df)\n",
    "    with_twitter_df = stock_df.merge(final_twitter, left_on='Datetime', right_on='timestamp').drop('timestamp',axis=1)\n",
    "    with_twitter_df['t+1'] = with_twitter_df['Adj Close'].shift(-1)\n",
    "    #4. Full Merge:\n",
    "    full_df = with_twitter_df.merge(headlines_final, left_on='Datetime', right_on='date_time').drop('date_time',axis=1)\n",
    "    full_df['t+1'] = full_df['Adj Close'].shift(-1)\n",
    "    #5. Evaluating Models:\n",
    "    lm_headlines_rmse, lm_headlines_r2, sgd_headlines_rmse, sgd_headlines_r2 = linear_modeling_headlines(with_headlines_df)\n",
    "    lm_twitter_rmse, lm_twitter_r2, sgd_twitter_rmse, sgd_twitter_r2 = linear_model_twitter(with_twitter_df)\n",
    "    lm_all_rmse, lm_all_r2, sgd_all_rmse, sgd_all_r2, xgb_all_rmse, xgb_all_r2, rf_all_rmse, rf_all_r2 = multi_model_full(full_df)\n",
    "    #6. Store in dict:\n",
    "    result_dict = {\n",
    "    'RMSE - Baseline':baseline_rmse, 'R2 - Baseline':baseline_r2, 'Linear RMSE - Baseline':lm_baseline_rmse, 'Linear R2 - Baseline':lm_baseline_r2, 'SGD RMSE - Baseline':sgd_baseline_rmse, 'SGD R2 - Baseline':sgd_baseline_r2,\n",
    "    'Linear RMSE - Only Headlines': lm_headlines_rmse, 'Linear R2 - Only Headlines':lm_headlines_r2, 'SGD RMSE - Only Headlines':sgd_headlines_rmse, 'SGD R2 - Only Headlines':sgd_headlines_r2,\n",
    "    'Linear RMSE - Only Twitter':lm_twitter_rmse, 'Linear R2 - Only Twitter':lm_twitter_r2, 'SGD RMSE - Only Twitter':sgd_twitter_rmse, 'SGD R2 - Only Twitter':sgd_twitter_r2,\n",
    "    'Linear RMSE - All':lm_all_rmse, 'Linear R2 - All':lm_all_r2, 'SGD RMSE - All':sgd_all_rmse, 'SGD R2 - All':sgd_all_r2, 'XGB RMSE - All':xgb_all_rmse, 'XGB R2 - All':xgb_all_r2, 'RF RMSE - All':rf_all_rmse,'RF R2 - All':rf_all_r2}\n",
    "    #7. Convert to DataFrame:\n",
    "    result_df = pd.DataFrame.from_dict(result_dict, orient='index', columns=['Values'])\n",
    "    #result_df.to_csv('~/LighthouseLabs-Final/Report_Analysis/AAPL_complete_analysis.csv')\n",
    "    return result_df, full_df"
   ]
  },
  {
   "source": [
    "result_df, full_df = evaluate_models(stock_df, headlines_df, twitter_df)\n",
    "result_df"
   ],
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "execution_count": 687,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                    Values\nRMSE - Baseline               6.213041e+00\nR2 - Baseline                 8.142633e-01\nLinear RMSE - Baseline        2.080228e+00\nLinear R2 - Baseline         -5.703734e+01\nSGD RMSE - Baseline           9.298627e+13\nSGD R2 - Baseline            -1.159641e+29\nLinear RMSE - Only Headlines  1.999692e+00\nLinear R2 - Only Headlines   -5.263052e+01\nSGD RMSE - Only Headlines     9.194230e+13\nSGD R2 - Only Headlines      -1.133748e+29\nLinear RMSE - Only Twitter    1.073135e+00\nLinear R2 - Only Twitter     -1.444522e+01\nSGD RMSE - Only Twitter       2.375640e+14\nSGD R2 - Only Twitter        -7.569146e+29\nLinear RMSE - All             1.107197e+00\nLinear R2 - All              -1.544129e+01\nSGD RMSE - All                2.375640e+14\nSGD R2 - All                 -7.569146e+29\nXGB RMSE - All                2.321654e+00\nXGB R2 - All                 -7.129045e+01\nRF RMSE - All                 2.025458e+00\nRF R2 - All                  -5.402150e+01",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Values</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>RMSE - Baseline</th>\n      <td>6.213041e+00</td>\n    </tr>\n    <tr>\n      <th>R2 - Baseline</th>\n      <td>8.142633e-01</td>\n    </tr>\n    <tr>\n      <th>Linear RMSE - Baseline</th>\n      <td>2.080228e+00</td>\n    </tr>\n    <tr>\n      <th>Linear R2 - Baseline</th>\n      <td>-5.703734e+01</td>\n    </tr>\n    <tr>\n      <th>SGD RMSE - Baseline</th>\n      <td>9.298627e+13</td>\n    </tr>\n    <tr>\n      <th>SGD R2 - Baseline</th>\n      <td>-1.159641e+29</td>\n    </tr>\n    <tr>\n      <th>Linear RMSE - Only Headlines</th>\n      <td>1.999692e+00</td>\n    </tr>\n    <tr>\n      <th>Linear R2 - Only Headlines</th>\n      <td>-5.263052e+01</td>\n    </tr>\n    <tr>\n      <th>SGD RMSE - Only Headlines</th>\n      <td>9.194230e+13</td>\n    </tr>\n    <tr>\n      <th>SGD R2 - Only Headlines</th>\n      <td>-1.133748e+29</td>\n    </tr>\n    <tr>\n      <th>Linear RMSE - Only Twitter</th>\n      <td>1.073135e+00</td>\n    </tr>\n    <tr>\n      <th>Linear R2 - Only Twitter</th>\n      <td>-1.444522e+01</td>\n    </tr>\n    <tr>\n      <th>SGD RMSE - Only Twitter</th>\n      <td>2.375640e+14</td>\n    </tr>\n    <tr>\n      <th>SGD R2 - Only Twitter</th>\n      <td>-7.569146e+29</td>\n    </tr>\n    <tr>\n      <th>Linear RMSE - All</th>\n      <td>1.107197e+00</td>\n    </tr>\n    <tr>\n      <th>Linear R2 - All</th>\n      <td>-1.544129e+01</td>\n    </tr>\n    <tr>\n      <th>SGD RMSE - All</th>\n      <td>2.375640e+14</td>\n    </tr>\n    <tr>\n      <th>SGD R2 - All</th>\n      <td>-7.569146e+29</td>\n    </tr>\n    <tr>\n      <th>XGB RMSE - All</th>\n      <td>2.321654e+00</td>\n    </tr>\n    <tr>\n      <th>XGB R2 - All</th>\n      <td>-7.129045e+01</td>\n    </tr>\n    <tr>\n      <th>RF RMSE - All</th>\n      <td>2.025458e+00</td>\n    </tr>\n    <tr>\n      <th>RF R2 - All</th>\n      <td>-5.402150e+01</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 687
    }
   ]
  },
  {
   "source": [
    "result_df.to_csv('~/LighthouseLabs-Final/Report_Analysis/TSLA_complete_analysis.csv')\n",
    "print('Saved!')"
   ],
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "execution_count": 688,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Saved!\n"
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}