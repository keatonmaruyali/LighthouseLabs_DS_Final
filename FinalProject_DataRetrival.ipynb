{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.9 64-bit ('base': conda)",
   "display_name": "Python 3.7.9 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "73a666d948711064f59d854e9125f239b700b6d583285c0d9848266fb8325020"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import tweepy\n",
    "import re\n",
    "from datetime import date,datetime,timedelta\n",
    "\n",
    "from urllib.request import urlopen, Request\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "import yfinance as yf"
   ]
  },
  {
   "source": [
    "## 1. Twitter API:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_for_hashtags(hashtag_phrase, date_since, end_date):\n",
    "    #auth = tweepy.OAuthHandler(\"psxikKIcDvu19SDl1qbTycOHY\", \"H2FV5HL4UyuzVUtANd0lo3HpsNzo1woDMuwlabVyv2E7g44SDb\")\n",
    "    #auth.set_access_token(\"1265371572942102528-XZyshdLemub7C7hrx51dyFrYCBCmvU\", \"CK4hmNukUYBhzr3CAtO3gvGFx7Ahr9TW3vZMjW6pl8yVd\")\n",
    "\n",
    "    auth = tweepy.OAuthHandler(\"4BSOA2XKS8vucCHyBjy502Aw8\", \"l27Zsdh9X9oGGff25gJ4PX9zN6ZnjYKonu2zISu17jsQlO5Dkb\")\n",
    "    auth.set_access_token(\"1265371572942102528-IsosHWjrXRDKHaqQFAbSPuM2FyH41k\", \"pVjYFAx8pDCBnJE48NxsA4KB6g4eNMw39TOWcTciLOB5u\")\n",
    "\n",
    "    #auth = tweepy.OAuthHandler(\"dQXGlGv8YFtPrZvQwcCnvbged\", \"ouKbQUg1dGubyKbp7DrNo45Qdv3nNzd7MyuvCgIKha0vpuNbDA\")\n",
    "    #auth.set_access_token(\"1265371572942102528-Jmu9hvd4yBep0KwV9U5mHFUnyUi9JV\", \"CYW3FRkyXJnSzRrHoN9FcLlBdSRHfA7GWJK1PMT7Q1S7P\")\n",
    "\n",
    "    #auth = tweepy.OAuthHandler(\"ljU1UWBCC0YNKlO9pwm1TshUc\", \"w5CDP6fNeNQhvDs075KiZQEIWI7VY1Z8BxiDc5kUAHTsXzOhCY\")\n",
    "    #auth.set_access_token(\"1309241898419322880-hBG39tjNql0FcHrjOMYg2qTN3TEtnw\", \"p0UOsVpTB9hu15R3YBMbWGw2zDpJjkUCmK0YadzA894ZF\")\n",
    "\n",
    "    api = tweepy.API(auth)\n",
    "\n",
    "    with open('%s.csv' % (hashtag_phrase[1:] + '_' + start_date), 'w+') as file:\n",
    "        w = csv.writer(file)\n",
    "        w.writerow(['timestamp', 'tweet_text', 'followers_count'])\n",
    "        for tweet in tweepy.Cursor(api.search, q=hashtag_phrase+' -filter:retweets', \\\n",
    "                                   lang=\"en\", tweet_mode='extended', since=start_date, until=end_date).items():\n",
    "            w.writerow([tweet.created_at, tweet.full_text.replace('\\n',' ').encode('utf-8'), tweet.user.followers_count])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#consumer_key = input('Consumer Key: ')\n",
    "#consumer_secret = input('Consumer Secret: ')\n",
    "#access_token = input('Access Token: ')\n",
    "#access_token_secret = input('Access Token Secret: ')\n",
    "    \n",
    "hashtag_phrase = input('Hashtag Phrase: ')\n",
    "start_date = input('Start Date: ')\n",
    "end_date = input('End Date: ')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    search_for_hashtags(hashtag_phrase, start_date, end_date)"
   ]
  },
  {
   "source": [
    "## 2. News Headlines:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news(ticker_code):\n",
    "    # 1. Define URL:\n",
    "    finwiz_url = 'https://finviz.com/quote.ashx?t='\n",
    "    # 2. Requesting data:\n",
    "    news_tables = {}\n",
    "    tickers = [ticker_code]\n",
    "    for ticker in tickers:\n",
    "        url = finwiz_url + ticker\n",
    "        req = Request(url=url,headers={'user-agent': 'my-app/0.0.1'}) \n",
    "        response = urlopen(req)    \n",
    "        # Read the contents of the file into 'html'\n",
    "        html = BeautifulSoup(response)\n",
    "        # Find 'news-table' in the Soup and load it into 'news_table'\n",
    "        news_table = html.find(id='news-table')\n",
    "        # Add the table to our dictionary\n",
    "        news_tables[ticker] = news_table\n",
    "    #3. Parsing news:\n",
    "    parsed_news = []\n",
    "    # Iterate through the news\n",
    "    for file_name, news_table in news_tables.items():\n",
    "        # Iterate through all tr tags in 'news_table'\n",
    "        for x in news_table.findAll('tr'):\n",
    "            # read the text from each tr tag into text\n",
    "            # get text from a only\n",
    "            text = x.a.get_text() \n",
    "            # splite text in the td tag into a list \n",
    "            date_scrape = x.td.text.split()\n",
    "            # if the length of 'date_scrape' is 1, load 'time' as the only element\n",
    "\n",
    "            if len(date_scrape) == 1:\n",
    "                time = date_scrape[0]\n",
    "                \n",
    "            # else load 'date' as the 1st element and 'time' as the second    \n",
    "            else:\n",
    "                date = date_scrape[0]\n",
    "                time = date_scrape[1]\n",
    "            # Extract the ticker from the file name, get the string up to the 1st '_'  \n",
    "            ticker = file_name.split('_')[0]\n",
    "            \n",
    "            # Append ticker, date, time and headline as a list to the 'parsed_news' list\n",
    "            parsed_news.append([ticker, date, time, text])\n",
    "\n",
    "    # 4. Split into columns and save:\n",
    "    # Instantiate the sentiment intensity analyzer\n",
    "    vader = SentimentIntensityAnalyzer()\n",
    "    # Set column names\n",
    "    columns = ['ticker', 'date', 'time', 'headline']\n",
    "    # Convert the parsed_news list into a DataFrame called 'parsed_and_scored_news'\n",
    "    parsed_and_scored_news = pd.DataFrame(parsed_news, columns=columns)\n",
    "    # Iterate through the headlines and get the polarity scores using vader\n",
    "    scores = parsed_and_scored_news['headline'].apply(vader.polarity_scores).tolist()\n",
    "    # Convert the 'scores' list of dicts into a DataFrame\n",
    "    scores_df = pd.DataFrame(scores)\n",
    "    # Join the DataFrames of the news and the list of dicts\n",
    "    parsed_and_scored_news = parsed_and_scored_news.join(scores_df, rsuffix='_right')\n",
    "    # Convert the date column from string to datetime\n",
    "    parsed_and_scored_news['date'] = pd.to_datetime(parsed_and_scored_news.date).dt.date\n",
    "    parsed_and_scored_news.to_csv('~/LighthouseLabs-Final/2. FinViz_Headline_Data/' + ticker + '_data_' + (datetime.today().strftime('%Y-%m-%d-%H-%M')) + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Done!\n"
    }
   ],
   "source": [
    "get_news('AAPL'), get_news('TSLA'), get_news('AMZN'), get_news('GS'), get_news('FB'), get_news('MSFT'), get_news('NVDA'), get_news('JNJ'), get_news('NFLX'), get_news('CVX'), get_news('PFE'), get_news('NKE'), get_news('GOOG')\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "source": [
    "## 3. Historical Stock Data:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stock_data(ticker):\n",
    "    start_date = '2020-09-23'\n",
    "    end_date = date.today()+timedelta(days=1)\n",
    "    # 1. Request data:\n",
    "    data = yf.download(ticker, \n",
    "                      start=start_date, \n",
    "                      end=end_date,\n",
    "                      interval='30m', \n",
    "                      progress=False)\n",
    "    # 2. Feature Engineering:\n",
    "    data['Percent Price Change Within Period'] = ((data['Close'] - data['Open'])/data['Open'])*100\n",
    "    data['Change in Close Price'] = data['Close'] - data['Close'].shift(1)\n",
    "    data['Scaled Volume'] = data['Volume']/data['Volume'].mean()\n",
    "    data_SMA = data['Adj Close'].rolling(window=3).mean().shift(1)\n",
    "    data['SMA(3)'] = data_SMA\n",
    "    data.reset_index(inplace=True)\n",
    "    data['Datetime']=data['Datetime'].dt.tz_convert('America/Montreal').dt.tz_localize(None)\n",
    "    #3. Export data:\n",
    "    f_name = ticker + \"_data\"\n",
    "    data.to_csv('~/LighthouseLabs-Final/1. Stock_Data/' + f_name + \".csv\")\n",
    "    print('Data saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Data saved!\nData saved!\nData saved!\nData saved!\nData saved!\nData saved!\nData saved!\nData saved!\nData saved!\nData saved!\nData saved!\nData saved!\nData saved!\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(None, None, None, None, None, None, None)"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "stock_data('AAPL'), stock_data('GOOG'), stock_data('FB'), stock_data('AMZN'), stock_data('NFLX'), stock_data('TSLA')\n",
    "\n",
    "stock_data('JNJ'), stock_data('MSFT'), stock_data('CVX'), stock_data('NKE'), stock_data('PFE'), stock_data('GS'), stock_data('NVDA')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}