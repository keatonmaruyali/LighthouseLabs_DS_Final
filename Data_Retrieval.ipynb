{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.9 64-bit ('base': conda)",
   "display_name": "Python 3.7.9 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "73a666d948711064f59d854e9125f239b700b6d583285c0d9848266fb8325020"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import tweepy\n",
    "import re\n",
    "from datetime import date,datetime,timedelta\n",
    "\n",
    "from urllib.request import urlopen, Request\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import numpy as np\n",
    "import yfinance as yf"
   ]
  },
  {
   "source": [
    "## 1. Twitter API:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_for_hashtags(hashtag_phrase, date_since, end_date):\n",
    "    auth = tweepy.OAuthHandler(\"psxikKIcDvu19SDl1qbTycOHY\", \"H2FV5HL4UyuzVUtANd0lo3HpsNzo1woDMuwlabVyv2E7g44SDb\")\n",
    "    auth.set_access_token(\"1265371572942102528-XZyshdLemub7C7hrx51dyFrYCBCmvU\", \"CK4hmNukUYBhzr3CAtO3gvGFx7Ahr9TW3vZMjW6pl8yVd\")\n",
    "\n",
    "    #auth = tweepy.OAuthHandler(\"4BSOA2XKS8vucCHyBjy502Aw8\", \"l27Zsdh9X9oGGff25gJ4PX9zN6ZnjYKonu2zISu17jsQlO5Dkb\")\n",
    "    #auth.set_access_token(\"1265371572942102528-IsosHWjrXRDKHaqQFAbSPuM2FyH41k\", \"pVjYFAx8pDCBnJE48NxsA4KB6g4eNMw39TOWcTciLOB5u\")\n",
    "\n",
    "    # auth = tweepy.OAuthHandler(\"dQXGlGv8YFtPrZvQwcCnvbged\", \"ouKbQUg1dGubyKbp7DrNo45Qdv3nNzd7MyuvCgIKha0vpuNbDA\")\n",
    "    # auth.set_access_token(\"1265371572942102528-Jmu9hvd4yBep0KwV9U5mHFUnyUi9JV\", \"CYW3FRkyXJnSzRrHoN9FcLlBdSRHfA7GWJK1PMT7Q1S7P\")\n",
    "\n",
    "    #auth = tweepy.OAuthHandler(\"ljU1UWBCC0YNKlO9pwm1TshUc\", \"w5CDP6fNeNQhvDs075KiZQEIWI7VY1Z8BxiDc5kUAHTsXzOhCY\")\n",
    "    #auth.set_access_token(\"1309241898419322880-hBG39tjNql0FcHrjOMYg2qTN3TEtnw\", \"p0UOsVpTB9hu15R3YBMbWGw2zDpJjkUCmK0YadzA894ZF\")\n",
    "\n",
    "    api = tweepy.API(auth)\n",
    "\n",
    "    with open('%s.csv' % (hashtag_phrase[1:] + '_' + start_date), 'w+') as file:\n",
    "        w = csv.writer(file)\n",
    "        w.writerow(['timestamp', 'tweet_text', 'followers_count'])\n",
    "        for tweet in tweepy.Cursor(api.search, q=hashtag_phrase+' -filter:retweets', \\\n",
    "                                   lang=\"en\", tweet_mode='extended', since=start_date, until=end_date).items():\n",
    "            w.writerow([tweet.created_at, tweet.full_text.replace('\\n',' ').encode('utf-8'), tweet.user.followers_count])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#consumer_key = input('Consumer Key: ')\n",
    "#consumer_secret = input('Consumer Secret: ')\n",
    "#access_token = input('Access Token: ')\n",
    "#access_token_secret = input('Access Token Secret: ')\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    hashtag_phrase = input('Hashtag Phrase: ')\n",
    "    start_date = input('Start Date: ')\n",
    "    end_date = input('End Date: ')\n",
    "    search_for_hashtags(hashtag_phrase, start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_pattern(input_txt, pattern):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for i in r:\n",
    "        input_txt = re.sub(i, '', input_txt)       \n",
    "    return input_txt\n",
    "    \n",
    "def clean_tweets(tweets):\n",
    "    #remove twitter Return handles (RT @xxx:)\n",
    "    tweets = np.vectorize(remove_pattern)(tweets, \"RT @[\\w]*:\") \n",
    "    #remove twitter handles (@xxx)\n",
    "    tweets = np.vectorize(remove_pattern)(tweets, \"@[\\w]*\")\n",
    "    #remove URL links (httpxxx)\n",
    "    tweets = np.vectorize(remove_pattern)(tweets, \"https?://[A-Za-z0-9./]*\")\n",
    "    #remove special characters, numbers, punctuations (except for #)\n",
    "    tweets = np.core.defchararray.replace(tweets, \"[^a-zA-Z]\", \" \")\n",
    "    return tweets\n",
    "\n",
    "def get_tweets(hashtag_phrase):\n",
    "    format_hashtag = '$'+hashtag_phrase\n",
    "    start_date = date.today()\n",
    "    end_date = date.today()+timedelta(days=1)\n",
    "    \n",
    "    # auth = tweepy.OAuthHandler(\"psxikKIcDvu19SDl1qbTycOHY\", \"H2FV5HL4UyuzVUtANd0lo3HpsNzo1woDMuwlabVyv2E7g44SDb\")\n",
    "    # auth.set_access_token(\"1265371572942102528-XZyshdLemub7C7hrx51dyFrYCBCmvU\", \"CK4hmNukUYBhzr3CAtO3gvGFx7Ahr9TW3vZMjW6pl8yVd\")\n",
    "\n",
    "    # auth = tweepy.OAuthHandler(\"4BSOA2XKS8vucCHyBjy502Aw8\", \"l27Zsdh9X9oGGff25gJ4PX9zN6ZnjYKonu2zISu17jsQlO5Dkb\")\n",
    "    # auth.set_access_token(\"1265371572942102528-IsosHWjrXRDKHaqQFAbSPuM2FyH41k\", \"pVjYFAx8pDCBnJE48NxsA4KB6g4eNMw39TOWcTciLOB5u\")\n",
    "\n",
    "    # auth = tweepy.OAuthHandler(\"dQXGlGv8YFtPrZvQwcCnvbged\", \"ouKbQUg1dGubyKbp7DrNo45Qdv3nNzd7MyuvCgIKha0vpuNbDA\")\n",
    "    # auth.set_access_token(\"1265371572942102528-Jmu9hvd4yBep0KwV9U5mHFUnyUi9JV\", \"CYW3FRkyXJnSzRrHoN9FcLlBdSRHfA7GWJK1PMT7Q1S7P\")\n",
    "\n",
    "    auth = tweepy.OAuthHandler(\"ljU1UWBCC0YNKlO9pwm1TshUc\", \"w5CDP6fNeNQhvDs075KiZQEIWI7VY1Z8BxiDc5kUAHTsXzOhCY\")\n",
    "    auth.set_access_token(\"1309241898419322880-hBG39tjNql0FcHrjOMYg2qTN3TEtnw\", \"p0UOsVpTB9hu15R3YBMbWGw2zDpJjkUCmK0YadzA894ZF\")\n",
    "\n",
    "    api = tweepy.API(auth)\n",
    "   \n",
    "    twitter_posts = pd.DataFrame(columns=['timestamp', 'tweet_text', 'followers_count'])\n",
    "    timestamp=[]\n",
    "    tweets=[]\n",
    "    follow_count=[]\n",
    "    # while True:\n",
    "    #     try:\n",
    "    for tweet in tweepy.Cursor(api.search, q=format_hashtag+' -filter:retweets', lang=\"en\", tweet_mode='extended',since=start_date, until=end_date).items():\n",
    "            timestamp.append(tweet.created_at)\n",
    "            tweets.append(tweet.full_text.replace('\\n',' ').encode('utf-8'))\n",
    "            follow_count.append(tweet.user.followers_count)\n",
    "        # except tweepy.TweepError:\n",
    "        #     break\n",
    "        # except StopIteration:\n",
    "        #     break\n",
    "    twitter_posts['timestamp']=timestamp\n",
    "    twitter_posts['tweet_text']=tweets\n",
    "    twitter_posts['followers_count']=follow_count\n",
    "    twitter_posts['tweet_text']=twitter_posts['tweet_text'].str.decode(\"utf-8\")\n",
    "    twitter_posts['scaled_followers_count'] =twitter_posts['followers_count']/twitter_posts['followers_count'].max()\n",
    "\n",
    "    vader = SentimentIntensityAnalyzer()\n",
    "    twitter_posts['tweet_text'] = clean_tweets(twitter_posts['tweet_text'])\n",
    "    # dataframe.reset_index(drop=False,inplace=True)\n",
    "    scores = twitter_posts['tweet_text'].apply(vader.polarity_scores).tolist()\n",
    "    scores_df = pd.DataFrame(scores)\n",
    "    df = twitter_posts.join(scores_df, rsuffix='_right')\n",
    "    df['compound'] = df['compound']*(twitter_posts['scaled_followers_count']+1)\n",
    "\n",
    "    df.to_csv('~/LighthouseLabs-Final/' + hashtag_phrase + '_' + (datetime.today().strftime('%Y-%m-%d')) + '.csv')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "              timestamp                                         tweet_text  \\\n",
       "0   2020-10-07 23:58:14  $NVDA  Arm exec says 'firewalls' will protect ...   \n",
       "1   2020-10-07 23:55:03  OptionAlarm provides Option Swing Trade Alerts...   \n",
       "2   2020-10-07 23:54:00  Real-time trading alerts from our team of seas...   \n",
       "3   2020-10-07 23:53:16  **Most profitable trading community. Get up to...   \n",
       "4   2020-10-07 23:50:05  I've  made 98k  with them . If you really want...   \n",
       "..                  ...                                                ...   \n",
       "345 2020-10-07 16:17:08  3/3 on all my chart ideas for today!   $OPRX +...   \n",
       "346 2020-10-07 16:15:08  $NVDA stock is cheap. It's going to go HAM dur...   \n",
       "347 2020-10-07 16:14:32                     $NVDA is the $GDX for #Bitcoin   \n",
       "348 2020-10-07 16:13:43  Most winning trading chat!!! For a limited tim...   \n",
       "349 2020-10-07 16:11:47  $JMIA Jumia Corp Breaking the 100ma, bullish m...   \n",
       "\n",
       "     followers_count  scaled_followers_count    neg    neu    pos  compound  \n",
       "0              25863                0.800415  0.000  0.702  0.298  1.348151  \n",
       "1               3060                0.094702  0.072  0.684  0.245  0.814130  \n",
       "2                113                0.003497  0.000  0.905  0.095  0.639127  \n",
       "3                132                0.004085  0.000  0.871  0.129  0.703161  \n",
       "4                132                0.004085  0.000  0.861  0.139  0.702558  \n",
       "..               ...                     ...    ...    ...    ...       ...  \n",
       "345              473                0.014639  0.000  1.000  0.000  0.000000  \n",
       "346             3934                0.121750  0.000  1.000  0.000  0.000000  \n",
       "347              130                0.004023  0.000  1.000  0.000  0.000000  \n",
       "348                0                0.000000  0.039  0.861  0.100  0.607400  \n",
       "349             1925                0.059575  0.000  1.000  0.000  0.000000  \n",
       "\n",
       "[350 rows x 8 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>timestamp</th>\n      <th>tweet_text</th>\n      <th>followers_count</th>\n      <th>scaled_followers_count</th>\n      <th>neg</th>\n      <th>neu</th>\n      <th>pos</th>\n      <th>compound</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2020-10-07 23:58:14</td>\n      <td>$NVDA  Arm exec says 'firewalls' will protect ...</td>\n      <td>25863</td>\n      <td>0.800415</td>\n      <td>0.000</td>\n      <td>0.702</td>\n      <td>0.298</td>\n      <td>1.348151</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2020-10-07 23:55:03</td>\n      <td>OptionAlarm provides Option Swing Trade Alerts...</td>\n      <td>3060</td>\n      <td>0.094702</td>\n      <td>0.072</td>\n      <td>0.684</td>\n      <td>0.245</td>\n      <td>0.814130</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2020-10-07 23:54:00</td>\n      <td>Real-time trading alerts from our team of seas...</td>\n      <td>113</td>\n      <td>0.003497</td>\n      <td>0.000</td>\n      <td>0.905</td>\n      <td>0.095</td>\n      <td>0.639127</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2020-10-07 23:53:16</td>\n      <td>**Most profitable trading community. Get up to...</td>\n      <td>132</td>\n      <td>0.004085</td>\n      <td>0.000</td>\n      <td>0.871</td>\n      <td>0.129</td>\n      <td>0.703161</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2020-10-07 23:50:05</td>\n      <td>I've  made 98k  with them . If you really want...</td>\n      <td>132</td>\n      <td>0.004085</td>\n      <td>0.000</td>\n      <td>0.861</td>\n      <td>0.139</td>\n      <td>0.702558</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>345</th>\n      <td>2020-10-07 16:17:08</td>\n      <td>3/3 on all my chart ideas for today!   $OPRX +...</td>\n      <td>473</td>\n      <td>0.014639</td>\n      <td>0.000</td>\n      <td>1.000</td>\n      <td>0.000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>346</th>\n      <td>2020-10-07 16:15:08</td>\n      <td>$NVDA stock is cheap. It's going to go HAM dur...</td>\n      <td>3934</td>\n      <td>0.121750</td>\n      <td>0.000</td>\n      <td>1.000</td>\n      <td>0.000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>347</th>\n      <td>2020-10-07 16:14:32</td>\n      <td>$NVDA is the $GDX for #Bitcoin</td>\n      <td>130</td>\n      <td>0.004023</td>\n      <td>0.000</td>\n      <td>1.000</td>\n      <td>0.000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>348</th>\n      <td>2020-10-07 16:13:43</td>\n      <td>Most winning trading chat!!! For a limited tim...</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>0.039</td>\n      <td>0.861</td>\n      <td>0.100</td>\n      <td>0.607400</td>\n    </tr>\n    <tr>\n      <th>349</th>\n      <td>2020-10-07 16:11:47</td>\n      <td>$JMIA Jumia Corp Breaking the 100ma, bullish m...</td>\n      <td>1925</td>\n      <td>0.059575</td>\n      <td>0.000</td>\n      <td>1.000</td>\n      <td>0.000</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>350 rows × 8 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "get_tweets('NVDA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsla,amzn,aapl,goog,fb,nflx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvx,gs,jnj,pfe,nke,msft,nvda"
   ]
  },
  {
   "source": [
    "## 2. News Headlines:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news(ticker_code):\n",
    "    # 1. Define URL:\n",
    "    finwiz_url = 'https://finviz.com/quote.ashx?t='\n",
    "    # 2. Requesting data:\n",
    "    news_tables = {}\n",
    "    tickers = [ticker_code]\n",
    "    for ticker in tickers:\n",
    "        url = finwiz_url + ticker\n",
    "        req = Request(url=url,headers={'user-agent': 'my-app/0.0.1'}) \n",
    "        response = urlopen(req)    \n",
    "        # Read the contents of the file into 'html'\n",
    "        html = BeautifulSoup(response)\n",
    "        # Find 'news-table' in the Soup and load it into 'news_table'\n",
    "        news_table = html.find(id='news-table')\n",
    "        # Add the table to our dictionary\n",
    "        news_tables[ticker] = news_table\n",
    "    #3. Parsing news:\n",
    "    parsed_news = []\n",
    "    # Iterate through the news\n",
    "    for file_name, news_table in news_tables.items():\n",
    "        # Iterate through all tr tags in 'news_table'\n",
    "        for x in news_table.findAll('tr'):\n",
    "            # read the text from each tr tag into text\n",
    "            # get text from a only\n",
    "            text = x.a.get_text() \n",
    "            # splite text in the td tag into a list \n",
    "            date_scrape = x.td.text.split()\n",
    "            # if the length of 'date_scrape' is 1, load 'time' as the only element\n",
    "\n",
    "            if len(date_scrape) == 1:\n",
    "                time = date_scrape[0]\n",
    "                \n",
    "            # else load 'date' as the 1st element and 'time' as the second    \n",
    "            else:\n",
    "                date = date_scrape[0]\n",
    "                time = date_scrape[1]\n",
    "            # Extract the ticker from the file name, get the string up to the 1st '_'  \n",
    "            ticker = file_name.split('_')[0]\n",
    "            \n",
    "            # Append ticker, date, time and headline as a list to the 'parsed_news' list\n",
    "            parsed_news.append([ticker, date, time, text])\n",
    "\n",
    "    # 4. Split into columns and save:\n",
    "    # Instantiate the sentiment intensity analyzer\n",
    "    vader = SentimentIntensityAnalyzer()\n",
    "    # Set column names\n",
    "    columns = ['ticker', 'date', 'time', 'headline']\n",
    "    # Convert the parsed_news list into a DataFrame called 'parsed_and_scored_news'\n",
    "    parsed_and_scored_news = pd.DataFrame(parsed_news, columns=columns)\n",
    "    # Iterate through the headlines and get the polarity scores using vader\n",
    "    scores = parsed_and_scored_news['headline'].apply(vader.polarity_scores).tolist()\n",
    "    # Convert the 'scores' list of dicts into a DataFrame\n",
    "    scores_df = pd.DataFrame(scores)\n",
    "    # Join the DataFrames of the news and the list of dicts\n",
    "    parsed_and_scored_news = parsed_and_scored_news.join(scores_df, rsuffix='_right')\n",
    "    # Convert the date column from string to datetime\n",
    "    parsed_and_scored_news['date'] = pd.to_datetime(parsed_and_scored_news.date).dt.date\n",
    "    parsed_and_scored_news.to_csv('~/LighthouseLabs-Final/Dataset/2. FinViz_Headline_Data/' + ticker + '_data_' + (datetime.today().strftime('%Y-%m-%d-%H')) + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "get_news('AAPL'), get_news('TSLA'), get_news('AMZN'), get_news('FB'), get_news('GOOG'), get_news('NFLX')\n",
    "\n",
    "get_news('MSFT'), get_news('NVDA'), get_news('JNJ'), get_news('CVX'), get_news('PFE'), get_news('NKE'),get_news('GS')\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "source": [
    "## 3. Historical Stock Data:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stock_data(ticker):\n",
    "    start_date = '2020-09-23'\n",
    "    end_date = date.today()+timedelta(days=1)\n",
    "    # 1. Request data:\n",
    "    data = yf.download(ticker, \n",
    "                      start=start_date, \n",
    "                      end=end_date,\n",
    "                      interval='30m', \n",
    "                      progress=False)\n",
    "    # 2. Feature Engineering:\n",
    "    data['Percent Price Change Within Period'] = ((data['Close'] - data['Open'])/data['Open'])*100\n",
    "    # data['Change in Close Price'] = data['Close'] - data['Close'].shift(1)\n",
    "    # data['Scaled Delta Close'] = data['Change in Close Price']/(data['Close'].mean())\n",
    "    data['Scaled Volume'] = data['Volume']/data['Volume'].mean()\n",
    "    data_SMA = data['Adj Close'].rolling(window=3).mean().shift(1)\n",
    "    data['SMA(3)'] = data_SMA\n",
    "    data['t+1'] = data['Adj Close'].shift(-1)\n",
    "    data.reset_index(inplace=True)\n",
    "    data['Datetime']=data['Datetime'].dt.tz_convert('America/Montreal').dt.tz_localize(None)\n",
    "    data.drop(['Open','High','Low','Close'],axis=1,inplace=True)\n",
    "    #3. Export data:\n",
    "    f_name = ticker + \"_data\"\n",
    "    data.to_csv('~/LighthouseLabs-Final/Dataset/1. Stock_Data/' + f_name + \".csv\")\n",
    "    print('Data saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Data saved!\n"
    }
   ],
   "source": [
    "stock_data('AAPL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Data saved!\n",
      "Data saved!\n",
      "Data saved!\n",
      "Data saved!\n",
      "Data saved!\n",
      "Data saved!\n",
      "Data saved!\n",
      "Data saved!\n",
      "Data saved!\n",
      "Data saved!\n",
      "Data saved!\n",
      "Data saved!\n",
      "Data saved!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(None, None, None, None, None, None, None)"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "stock_data('AAPL'), stock_data('GOOG'), stock_data('FB'), stock_data('AMZN'), stock_data('NFLX'), stock_data('TSLA')\n",
    "\n",
    "stock_data('JNJ'), stock_data('MSFT'), stock_data('CVX'), stock_data('NKE'), stock_data('PFE'), stock_data('GS'), stock_data('NVDA')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}