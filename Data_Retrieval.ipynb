{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.9 64-bit ('base': conda)",
   "display_name": "Python 3.7.9 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "73a666d948711064f59d854e9125f239b700b6d583285c0d9848266fb8325020"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import tweepy\n",
    "import re\n",
    "from datetime import date,datetime,timedelta\n",
    "\n",
    "from urllib.request import urlopen, Request\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import numpy as np\n",
    "import yfinance as yf"
   ]
  },
  {
   "source": [
    "## 1. Twitter API:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_pattern(input_txt, pattern):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for i in r:\n",
    "        input_txt = re.sub(i, '', input_txt)       \n",
    "    return input_txt\n",
    "    \n",
    "def clean_tweets(tweets):\n",
    "    #remove twitter Return handles (RT @xxx:)\n",
    "    tweets = np.vectorize(remove_pattern)(tweets, \"RT @[\\w]*:\") \n",
    "    #remove twitter handles (@xxx)\n",
    "    tweets = np.vectorize(remove_pattern)(tweets, \"@[\\w]*\")\n",
    "    #remove URL links (httpxxx)\n",
    "    tweets = np.vectorize(remove_pattern)(tweets, \"https?://[A-Za-z0-9./]*\")\n",
    "    #remove special characters, numbers, punctuations (except for #)\n",
    "    tweets = np.core.defchararray.replace(tweets, \"[^a-zA-Z]\", \" \")\n",
    "    return tweets\n",
    "\n",
    "def get_tweets(hashtag_phrase):\n",
    "    format_hashtag = '$'+hashtag_phrase\n",
    "    start_date = date.today()\n",
    "    end_date = date.today()+timedelta(days=1)\n",
    "    \n",
    "    consumer_key = os.environ['consumer_key']\n",
    "    consumer_secret = os.environ['consumer_secret']\n",
    "    access_token = os.environ['twitter_access_token']\n",
    "    access_token_secret = os.environ['twitter_access_secret']\n",
    "\n",
    "    auth = tweepy.OAuthHandler(consumer_key,consumer_secret)\n",
    "    auth.set_access_token(access_token,access_token_secret)\n",
    "\n",
    "    api = tweepy.API(auth)\n",
    "   \n",
    "    twitter_posts = pd.DataFrame(columns=['timestamp', 'tweet_text', 'followers_count'])\n",
    "    timestamp=[]\n",
    "    tweets=[]\n",
    "    follow_count=[]\n",
    "    # while True:\n",
    "    #     try:\n",
    "    for tweet in tweepy.Cursor(api.search, q=format_hashtag+' -filter:retweets', lang=\"en\", tweet_mode='extended',since=start_date, until=end_date).items():\n",
    "            timestamp.append(tweet.created_at)\n",
    "            tweets.append(tweet.full_text.replace('\\n',' ').encode('utf-8'))\n",
    "            follow_count.append(tweet.user.followers_count)\n",
    "        # except tweepy.TweepError:\n",
    "        #     break\n",
    "        # except StopIteration:\n",
    "        #     break\n",
    "    twitter_posts['timestamp']=timestamp\n",
    "    twitter_posts['tweet_text']=tweets\n",
    "    twitter_posts['followers_count']=follow_count\n",
    "    twitter_posts['tweet_text']=twitter_posts['tweet_text'].str.decode(\"utf-8\")\n",
    "    twitter_posts['scaled_followers_count'] =twitter_posts['followers_count']/twitter_posts['followers_count'].max()\n",
    "\n",
    "    vader = SentimentIntensityAnalyzer()\n",
    "    twitter_posts['tweet_text'] = clean_tweets(twitter_posts['tweet_text'])\n",
    "    # dataframe.reset_index(drop=False,inplace=True)\n",
    "    scores = twitter_posts['tweet_text'].apply(vader.polarity_scores).tolist()\n",
    "    scores_df = pd.DataFrame(scores)\n",
    "    df = twitter_posts.join(scores_df, rsuffix='_right')\n",
    "    df['compound'] = df['compound']*(twitter_posts['scaled_followers_count']+1)\n",
    "\n",
    "    df.to_csv('~/LighthouseLabs-Final/' + hashtag_phrase + '_' + (datetime.today().strftime('%Y-%m-%d')) + '.csv')\n",
    "    return df"
   ]
  },
  {
   "source": [
    "## 2. News Headlines:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news(ticker_code):\n",
    "    # 1. Define URL:\n",
    "    finwiz_url = 'https://finviz.com/quote.ashx?t='\n",
    "    # 2. Requesting data:\n",
    "    news_tables = {}\n",
    "    tickers = [ticker_code]\n",
    "    for ticker in tickers:\n",
    "        url = finwiz_url + ticker\n",
    "        req = Request(url=url,headers={'user-agent': 'my-app/0.0.1'}) \n",
    "        response = urlopen(req)    \n",
    "        # Read the contents of the file into 'html'\n",
    "        html = BeautifulSoup(response)\n",
    "        # Find 'news-table' in the Soup and load it into 'news_table'\n",
    "        news_table = html.find(id='news-table')\n",
    "        # Add the table to our dictionary\n",
    "        news_tables[ticker] = news_table\n",
    "    #3. Parsing news:\n",
    "    parsed_news = []\n",
    "    # Iterate through the news\n",
    "    for file_name, news_table in news_tables.items():\n",
    "        # Iterate through all tr tags in 'news_table'\n",
    "        for x in news_table.findAll('tr'):\n",
    "            # read the text from each tr tag into text\n",
    "            # get text from a only\n",
    "            text = x.a.get_text() \n",
    "            # splite text in the td tag into a list \n",
    "            date_scrape = x.td.text.split()\n",
    "            # if the length of 'date_scrape' is 1, load 'time' as the only element\n",
    "\n",
    "            if len(date_scrape) == 1:\n",
    "                time = date_scrape[0]\n",
    "                \n",
    "            # else load 'date' as the 1st element and 'time' as the second    \n",
    "            else:\n",
    "                date = date_scrape[0]\n",
    "                time = date_scrape[1]\n",
    "            # Extract the ticker from the file name, get the string up to the 1st '_'  \n",
    "            ticker = file_name.split('_')[0]\n",
    "            \n",
    "            # Append ticker, date, time and headline as a list to the 'parsed_news' list\n",
    "            parsed_news.append([ticker, date, time, text])\n",
    "\n",
    "    # 4. Split into columns and save:\n",
    "    # Instantiate the sentiment intensity analyzer\n",
    "    vader = SentimentIntensityAnalyzer()\n",
    "    # Set column names\n",
    "    columns = ['ticker', 'date', 'time', 'headline']\n",
    "    # Convert the parsed_news list into a DataFrame called 'parsed_and_scored_news'\n",
    "    parsed_and_scored_news = pd.DataFrame(parsed_news, columns=columns)\n",
    "    # Iterate through the headlines and get the polarity scores using vader\n",
    "    scores = parsed_and_scored_news['headline'].apply(vader.polarity_scores).tolist()\n",
    "    # Convert the 'scores' list of dicts into a DataFrame\n",
    "    scores_df = pd.DataFrame(scores)\n",
    "    # Join the DataFrames of the news and the list of dicts\n",
    "    parsed_and_scored_news = parsed_and_scored_news.join(scores_df, rsuffix='_right')\n",
    "    # Convert the date column from string to datetime\n",
    "    parsed_and_scored_news['date'] = pd.to_datetime(parsed_and_scored_news.date).dt.date\n",
    "    parsed_and_scored_news.to_csv('~/LighthouseLabs-Final/Dataset/2. FinViz_Headline_Data/' + ticker + '_data_' + (datetime.today().strftime('%Y-%m-%d-%H')) + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "get_news('AAPL'), get_news('TSLA'), get_news('AMZN'), get_news('FB'), get_news('GOOG'), get_news('NFLX')\n",
    "\n",
    "get_news('MSFT'), get_news('NVDA'), get_news('JNJ'), get_news('CVX'), get_news('PFE'), get_news('NKE'),get_news('GS')\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "source": [
    "## 3. Historical Stock Data:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stock_data(ticker):\n",
    "    start_date = '2020-09-23'\n",
    "    end_date = date.today()+timedelta(days=1)\n",
    "    # 1. Request data:\n",
    "    data = yf.download(ticker, \n",
    "                      start=start_date, \n",
    "                      end=end_date,\n",
    "                      interval='30m', \n",
    "                      progress=False)\n",
    "    # 2. Feature Engineering:\n",
    "    data['Percent Price Change Within Period'] = ((data['Close'] - data['Open'])/data['Open'])*100\n",
    "    # data['Change in Close Price'] = data['Close'] - data['Close'].shift(1)\n",
    "    # data['Scaled Delta Close'] = data['Change in Close Price']/(data['Close'].mean())\n",
    "    data['Scaled Volume'] = data['Volume']/data['Volume'].mean()\n",
    "    data_SMA = data['Adj Close'].rolling(window=3).mean().shift(1)\n",
    "    data['SMA(3)'] = data_SMA\n",
    "    data['t+1'] = data['Adj Close'].shift(-1)\n",
    "    data.reset_index(inplace=True)\n",
    "    data['Datetime']=data['Datetime'].dt.tz_convert('America/Montreal').dt.tz_localize(None)\n",
    "    data.drop(['Open','High','Low','Close'],axis=1,inplace=True)\n",
    "    #3. Export data:\n",
    "    f_name = ticker + \"_data\"\n",
    "    # data.to_csv('~/LighthouseLabs-Final/Dataset/1. Stock_Data/' + f_name + \".csv\")\n",
    "    # print('Data saved!')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Data saved!\n",
      "Data saved!\n",
      "Data saved!\n",
      "Data saved!\n",
      "Data saved!\n",
      "Data saved!\n",
      "Data saved!\n",
      "Data saved!\n",
      "Data saved!\n",
      "Data saved!\n",
      "Data saved!\n",
      "Data saved!\n",
      "Data saved!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(None, None, None, None, None, None, None)"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "stock_data('AAPL'), stock_data('GOOG'), stock_data('FB'), stock_data('AMZN'), stock_data('NFLX'), stock_data('TSLA')\n",
    "\n",
    "stock_data('JNJ'), stock_data('MSFT'), stock_data('CVX'), stock_data('NKE'), stock_data('PFE'), stock_data('GS'), stock_data('NVDA')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}