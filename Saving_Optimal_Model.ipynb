{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.9 64-bit ('base': conda)",
   "display_name": "Python 3.7.9 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "73a666d948711064f59d854e9125f239b700b6d583285c0d9848266fb8325020"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "# from sklearn.linear_model import SGDRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "# import re\n",
    "# from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_change_sentiment(data):\n",
    "    change_in_sent = []\n",
    "    change_in_sent.append(data['compound'][0])\n",
    "    for i in range(1,len(data['compound'])):\n",
    "        if data['compound'][i] == 0:\n",
    "            change_in_sent.append(0)\n",
    "        elif data['compound'][i] < 0 or data['compound'][i] > 0:\n",
    "            dif = data['compound'][i] - data['compound'][(i-1)]\n",
    "            change_in_sent.append(dif)\n",
    "    return change_in_sent\n",
    "\n",
    "def remove_pattern(input_txt, pattern):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for i in r:\n",
    "        input_txt = re.sub(i, '', input_txt)       \n",
    "    return input_txt\n",
    "    \n",
    "def clean_tweets(tweets):\n",
    "    #remove twitter Return handles (RT @xxx:)\n",
    "    tweets = np.vectorize(remove_pattern)(tweets, \"RT @[\\w]*:\") \n",
    "    #remove twitter handles (@xxx)\n",
    "    tweets = np.vectorize(remove_pattern)(tweets, \"@[\\w]*\")\n",
    "    #remove URL links (httpxxx)\n",
    "    tweets = np.vectorize(remove_pattern)(tweets, \"https?://[A-Za-z0-9./]*\")   \n",
    "    tweets = np.vectorize(remove_pattern)(tweets, \"b'\")\n",
    "    tweets = np.vectorize(remove_pattern)(tweets, 'b\"')\n",
    "    #remove special characters, numbers, punctuations (except for #)\n",
    "    tweets = np.core.defchararray.replace(tweets, \"[^a-zA-Z]\", \" \")\n",
    "    return tweets\n",
    "\n",
    "\n",
    "def classify_news(dataframe):\n",
    "    day23, day24, day25, day26, day27, day28, day29, day30, day31, day32, day33, day34, day35, day36, day37, day38 = [],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]\n",
    "\n",
    "    for i in range(len(dataframe['timestamp'])):\n",
    "        if dataframe['timestamp'][i].day == 23 and (dataframe['timestamp'][i].hour <= 15 and dataframe['timestamp'][i].hour >= 9):\n",
    "            day23.append(i)\n",
    "        elif dataframe['timestamp'][i].day == 24 and (dataframe['timestamp'][i].hour <= 15 and dataframe['timestamp'][i].hour >= 9):\n",
    "            day24.append(i)       \n",
    "        elif dataframe['timestamp'][i].day == 25 and (dataframe['timestamp'][i].hour <= 15 and dataframe['timestamp'][i].hour >= 9):\n",
    "            day25.append(i)\n",
    "        elif dataframe['timestamp'][i].day == 26 and (dataframe['timestamp'][i].hour <= 15 and dataframe['timestamp'][i].hour >= 9):\n",
    "            day26.append(i)\n",
    "        elif dataframe['timestamp'][i].day == 27 and (dataframe['timestamp'][i].hour <= 15 and dataframe['timestamp'][i].hour >= 9):\n",
    "            day27.append(i)\n",
    "        elif dataframe['timestamp'][i].day == 28 and (dataframe['timestamp'][i].hour <= 15 and dataframe['timestamp'][i].hour >= 9):\n",
    "            day28.append(i)\n",
    "        elif dataframe['timestamp'][i].day == 29 and (dataframe['timestamp'][i].hour <= 15 and dataframe['timestamp'][i].hour >= 9):\n",
    "            day29.append(i)\n",
    "        elif dataframe['timestamp'][i].day == 30 and (dataframe['timestamp'][i].hour <= 15 and dataframe['timestamp'][i].hour >= 9):\n",
    "            day30.append(i)\n",
    "        elif dataframe['timestamp'][i].day == 1 and (dataframe['timestamp'][i].hour <= 15 and dataframe['timestamp'][i].hour >= 9):\n",
    "            day31.append(i)\n",
    "        elif dataframe['timestamp'][i].day == 2 and (dataframe['timestamp'][i].hour <= 15 and dataframe['timestamp'][i].hour >= 9):\n",
    "            day32.append(i)\n",
    "        elif dataframe['timestamp'][i].day == 3 and (dataframe['timestamp'][i].hour <= 15 and dataframe['timestamp'][i].hour >= 9):\n",
    "            day33.append(i)\n",
    "        elif dataframe['timestamp'][i].day == 4 and (dataframe['timestamp'][i].hour <= 15 and dataframe['timestamp'][i].hour >= 9):\n",
    "            day34.append(i)\n",
    "        elif dataframe['timestamp'][i].day == 5 and (dataframe['timestamp'][i].hour <= 15 and dataframe['timestamp'][i].hour >= 9):\n",
    "            day35.append(i)\n",
    "        elif dataframe['timestamp'][i].day == 6 and (dataframe['timestamp'][i].hour <= 15 and dataframe['timestamp'][i].hour >= 9):\n",
    "            day36.append(i)\n",
    "        elif dataframe['timestamp'][i].day == 7 and (dataframe['timestamp'][i].hour <= 15 and dataframe['timestamp'][i].hour >= 9):\n",
    "            day37.append(i)\n",
    "        elif dataframe['timestamp'][i].day == 8 and (dataframe['timestamp'][i].hour <= 15 and dataframe['timestamp'][i].hour >= 9):\n",
    "            day38.append(i)\n",
    "        else:\n",
    "            pass\n",
    "    news_d23,news_d24,news_d25,news_d26,news_d27,news_d28,news_d29,news_d30,news_d31,news_d32,news_d33,news_d34,news_d35,news_d36,news_d37,news_d38 = dataframe.iloc[day23],dataframe.iloc[day24],dataframe.iloc[day25], dataframe.iloc[day26], dataframe.iloc[day27],dataframe.iloc[day28],dataframe.iloc[day29],dataframe.iloc[day30],dataframe.iloc[day31], dataframe.iloc[day32],dataframe.iloc[day33],dataframe.iloc[day34],dataframe.iloc[day35],dataframe.iloc[day36],dataframe.iloc[day37],dataframe.iloc[day38]\n",
    "    return news_d23,news_d24,news_d25,news_d26,news_d27,news_d28,news_d29,news_d30,news_d31,news_d32,news_d33,news_d34,news_d35,news_d36,news_d37,news_d38\n",
    "\n",
    "\n",
    "def preprocess_headlines(data):\n",
    "    data.drop_duplicates(subset='headline',keep=False, inplace=True)\n",
    "    data.drop(['ticker','neg','neu','pos'], axis=1, inplace=True)\n",
    "    data.rename(columns={'date_time':'timestamp'},inplace=True)\n",
    "    data.set_index('timestamp', inplace=True)\n",
    "    \n",
    "    data_30m = data.resample('30min').median().ffill().reset_index()\n",
    "    headline_sma = data_30m['compound'].rolling(3).mean()\n",
    "    data_30m['Compound SMA(3) Headlines'] = headline_sma\n",
    "    change_in_sent=calc_change_sentiment(data_30m)\n",
    "    data_30m['change in sentiment headlines'] = change_in_sent\n",
    "    data_30m['change in sentiment headlines (t-1)'] = data_30m['change in sentiment headlines'].shift(1)\n",
    "    # Splitting the headlines into days.\n",
    "    news_d23,news_d24,news_d25,news_d26,news_d27,news_d28,news_d29,news_d30,news_d31,news_d32,news_d33,news_d34,news_d35,news_d36,news_d37,news_d38 = classify_news(data_30m)\n",
    "    # Removing the first row because the time is 9:00am, which will not align with the stock data, which starts at 9:30am.\n",
    "    news_d23_red,news_d24_red, news_d25_red, news_d28_red,news_d29_red,news_d30_red,news_d31_red,news_d32_red,news_d35_red,news_d36_red,news_d37_red,news_d38_red = news_d23.iloc[4:],news_d24.iloc[1:],news_d25.iloc[1:],news_d28.iloc[1:],news_d29.iloc[1:],news_d30.iloc[1:],news_d31.iloc[1:],news_d32.iloc[1:],news_d35.iloc[1:],news_d36.iloc[1:],news_d37.iloc[1:],news_d38.iloc[1:]\n",
    "    # Merge the days.\n",
    "    frames_news = [news_d23_red,news_d24_red, news_d25_red, news_d28_red,news_d29_red,news_d30_red,news_d31_red,news_d32_red,news_d35_red,news_d36_red,news_d37_red,news_d38_red]\n",
    "    processed_headlines = pd.concat(frames_news)\n",
    "    return processed_headlines\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_posts(dataframe):\n",
    "    dataframe.drop(['neg','neu','pos','followers_count'],axis=1,inplace=True)\n",
    "    dataframe['timestamp'] = dataframe['timestamp'].dt.tz_localize('UTC').dt.tz_convert('America/Montreal').dt.tz_localize(None)\n",
    "    dataframe.set_index('timestamp', inplace=True)\n",
    "    \n",
    "    twitter_df_30m = dataframe.resample('30min').median().ffill().reset_index()\n",
    "    change_in_sent = calc_change_sentiment(twitter_df_30m)\n",
    "    twitter_sma = twitter_df_30m['compound'].rolling(3).mean()\n",
    "    twitter_df_30m['Compound SMA(3) Twitter'] = twitter_sma\n",
    "    twitter_df_30m['change in sentiment twitter'] = change_in_sent\n",
    "    twitter_df_30m['change in sentiment twitter (t-1)'] = twitter_df_30m['change in sentiment twitter'].shift(1)\n",
    "\n",
    "    tw_news_d23,tw_news_d24,tw_news_d25,tw_news_d26,tw_news_d27,tw_news_d28,tw_news_d29,tw_news_d30,tw_news_d31,tw_news_d32,tw_news_d33,tw_news_d34,tw_news_d35,tw_news_d36,tw_news_d37,tw_news_d38 = classify_news(twitter_df_30m)\n",
    "\n",
    "    tw_news_d23_30m,tw_news_d24_30m,tw_news_d25_30m, tw_news_d28_30m,tw_news_d29_30m,tw_news_d30_30m,tw_news_d31_30m,tw_news_d32_30m,tw_news_d35_30m,tw_news_d36_30m,tw_news_d37_30m,tw_news_d38_30m = tw_news_d23.iloc[4:],tw_news_d24.iloc[1:],tw_news_d25.iloc[1:],tw_news_d28.iloc[1:],tw_news_d29.iloc[1:],tw_news_d30.iloc[1:],tw_news_d31.iloc[1:],tw_news_d32.iloc[1:],tw_news_d35.iloc[1:],tw_news_d36.iloc[1:],tw_news_d37.iloc[1:],tw_news_d38.iloc[1:]\n",
    "\n",
    "    frames = [tw_news_d23_30m,tw_news_d24_30m,tw_news_d25_30m,tw_news_d28_30m,tw_news_d29_30m,tw_news_d30_30m,tw_news_d31_30m,tw_news_d32_30m,tw_news_d35_30m,tw_news_d36_30m,tw_news_d37_30m,tw_news_d38_30m]\n",
    "    processed_tweets = pd.concat(frames)\n",
    "    return processed_tweets\n",
    "\n",
    "\n",
    "def cleaning_df(stock_df, headline_df, twitter_df):\n",
    "    headlines_final = preprocess_headlines(headline_df)\n",
    "    with_headlines_df = stock_df.merge(headlines_final, left_on='Datetime', right_on='timestamp').drop('timestamp',axis=1)\n",
    "    with_headlines_df['t+1'] = with_headlines_df['Adj Close'].shift(-1)\n",
    "    #3. Twitter Final Merge:\n",
    "    final_twitter = preprocess_posts(twitter_df)\n",
    "    with_twitter_df = stock_df.merge(final_twitter, left_on='Datetime', right_on='timestamp').drop('timestamp',axis=1)\n",
    "    with_twitter_df['t+1'] = with_twitter_df['Adj Close'].shift(-1)\n",
    "    #4. Full Merge:\n",
    "    full_df = with_twitter_df.merge(headlines_final, left_on='Datetime', right_on='timestamp').drop('timestamp',axis=1)\n",
    "    full_df['Percent Price Change Within Period (t+1)'] = full_df['Percent Price Change Within Period'].shift(-1)\n",
    "\n",
    "    return with_headlines_df,with_twitter_df,full_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_model_full(dataframe):\n",
    "    x_var = ['Adj Close','Scaled Volume','compound_y','compound_x','Compound SMA(3) Headlines','Compound SMA(3) Twitter','SMA(3)','change in sentiment headlines','change in sentiment headlines (t-1)','change in sentiment twitter','change in sentiment twitter (t-1)']\n",
    "    i = len(dataframe['Percent Price Change Within Period (t+1)'])-4\n",
    "    y_train, y_test = dataframe['Percent Price Change Within Period (t+1)'][:i], dataframe['Percent Price Change Within Period (t+1)'][i:-1]\n",
    "    X_train, X_test = dataframe[x_var][:i], dataframe[x_var][i:-1]\n",
    "\n",
    "    xg_reg = xgb.XGBRegressor(colsample_bytree= 0.3, gamma= 0.0, learning_rate= 0.2, max_depth= 5, n_estimators= 20000)\n",
    "    xg_reg.fit(X_train,y_train)\n",
    "    preds3 = xg_reg.predict(X_test)\n",
    "    # svr = SVR(kernel='rbf', C=0.01, epsilon=0.001)\n",
    "    # svr.fit(X_train,y_train)\n",
    "    # preds3 = svr.predict(X_test)\n",
    "    rmse3 = np.sqrt(mean_squared_error(y_test, preds3))\n",
    "    filename = 'finalized_xgb_model.sav'\n",
    "    pickle.dump(xg_reg, open(filename, 'wb'))\n",
    "    print('Model Saved!')\n",
    "    print('RMSE Score: ',rmse3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data(ticker,ticker2,ticker3,ticker4,ticker5,ticker6,ticker7,ticker8,ticker9,ticker10,ticker11,ticker12,ticker13):\n",
    "    stock_path = '~/LighthouseLabs-Final/Dataset/1. Stock_Data/'\n",
    "    headline_path = '~/LighthouseLabs-Final/Dataset/2. FinViz_Headline_Data/'\n",
    "    twitter_path = '~/LighthouseLabs-Final/Dataset/3. Twitter_Data/'\n",
    "    latest_headlines='10-07'\n",
    "    # 1. Historical Stock Data:------------------------------------------------------------------------------------------\n",
    "    stock_df1 = pd.read_csv(stock_path+ticker+'_data.csv', index_col=0,parse_dates=['Datetime'])\n",
    "    stock_df2 = pd.read_csv(stock_path+ticker2+'_data.csv',index_col=0, parse_dates=['Datetime'])\n",
    "    stock_df3 = pd.read_csv(stock_path+ticker3+'_data.csv',index_col=0, parse_dates=['Datetime'])\n",
    "    stock_df4 = pd.read_csv(stock_path+ticker4+'_data.csv',index_col=0, parse_dates=['Datetime'])\n",
    "    stock_df5 = pd.read_csv(stock_path+ticker4+'_data.csv',index_col=0, parse_dates=['Datetime'])\n",
    "    stock_df6 = pd.read_csv(stock_path+ticker4+'_data.csv',index_col=0, parse_dates=['Datetime'])\n",
    "    stock_df7 = pd.read_csv(stock_path+ticker4+'_data.csv',index_col=0, parse_dates=['Datetime'])\n",
    "    stock_df8 = pd.read_csv(stock_path+ticker4+'_data.csv',index_col=0, parse_dates=['Datetime'])\n",
    "    stock_df9 = pd.read_csv(stock_path+ticker4+'_data.csv',index_col=0, parse_dates=['Datetime'])\n",
    "    stock_df10 = pd.read_csv(stock_path+ticker4+'_data.csv',index_col=0, parse_dates=['Datetime'])\n",
    "    stock_df11 = pd.read_csv(stock_path+ticker4+'_data.csv',index_col=0, parse_dates=['Datetime'])\n",
    "    stock_df12 = pd.read_csv(stock_path+ticker4+'_data.csv',index_col=0, parse_dates=['Datetime'])\n",
    "    stock_df13 = pd.read_csv(stock_path+ticker4+'_data.csv',index_col=0, parse_dates=['Datetime'])\n",
    "\n",
    "    # 2. Headline Data: ----------------------------------------------------------------------------------------------------\n",
    "    headlines1 = pd.read_csv(headline_path+ticker+'_2020-09-23_2020-'+latest_headlines+'.csv', index_col=0, parse_dates=['date_time'])\n",
    "    headlines2 = pd.read_csv(headline_path+ticker2+'_2020-09-23_2020-'+latest_headlines+'.csv', index_col=0, parse_dates=['date_time'])\n",
    "    headlines3 = pd.read_csv(headline_path+ticker3+'_2020-09-23_2020-'+latest_headlines+'.csv', index_col=0, parse_dates=['date_time'])\n",
    "    headlines4 = pd.read_csv(headline_path+ticker4+'_2020-09-23_2020-'+latest_headlines+'.csv', index_col=0, parse_dates=['date_time'])\n",
    "    headlines5 = pd.read_csv(headline_path+ticker4+'_2020-09-23_2020-'+latest_headlines+'.csv', index_col=0, parse_dates=['date_time'])\n",
    "    headlines6 = pd.read_csv(headline_path+ticker4+'_2020-09-23_2020-'+latest_headlines+'.csv', index_col=0, parse_dates=['date_time'])\n",
    "    headlines7 = pd.read_csv(headline_path+ticker4+'_2020-09-23_2020-'+latest_headlines+'.csv', index_col=0, parse_dates=['date_time'])\n",
    "    headlines8 = pd.read_csv(headline_path+ticker4+'_2020-09-23_2020-'+latest_headlines+'.csv', index_col=0, parse_dates=['date_time'])\n",
    "    headlines9 = pd.read_csv(headline_path+ticker4+'_2020-09-23_2020-'+latest_headlines+'.csv', index_col=0, parse_dates=['date_time'])\n",
    "    headlines10 = pd.read_csv(headline_path+ticker4+'_2020-09-23_2020-'+latest_headlines+'.csv', index_col=0, parse_dates=['date_time'])\n",
    "    headlines11 = pd.read_csv(headline_path+ticker4+'_2020-09-23_2020-'+latest_headlines+'.csv', index_col=0, parse_dates=['date_time'])\n",
    "    headlines12 = pd.read_csv(headline_path+ticker4+'_2020-09-23_2020-'+latest_headlines+'.csv', index_col=0, parse_dates=['date_time'])\n",
    "    headlines13 = pd.read_csv(headline_path+ticker4+'_2020-09-23_2020-'+latest_headlines+'.csv', index_col=0, parse_dates=['date_time'])\n",
    "\n",
    "    # 3. Twitter Data:----------------------------------------------------------------------------------------------------\n",
    "    twitter1 = pd.read_csv(twitter_path+ticker+'_2020-09-23_2020-'+latest_headlines+'.csv', index_col=0,parse_dates=['timestamp'])\n",
    "    twitter2 = pd.read_csv(twitter_path+ticker2+'_2020-09-23_2020-'+latest_headlines+'.csv',index_col=0, parse_dates=['timestamp'])\n",
    "    twitter3 = pd.read_csv(twitter_path+ticker3+'_2020-09-23_2020-'+latest_headlines+'.csv',index_col=0, parse_dates=['timestamp'])\n",
    "    twitter4 = pd.read_csv(twitter_path+ticker4+'_2020-09-23_2020-'+latest_headlines+'.csv',index_col=0, parse_dates=['timestamp'])\n",
    "    twitter5 = pd.read_csv(twitter_path+ticker4+'_2020-09-23_2020-'+latest_headlines+'.csv',index_col=0, parse_dates=['timestamp'])\n",
    "    twitter6 = pd.read_csv(twitter_path+ticker4+'_2020-09-23_2020-'+latest_headlines+'.csv',index_col=0, parse_dates=['timestamp'])\n",
    "    twitter7 = pd.read_csv(twitter_path+ticker4+'_2020-09-23_2020-'+latest_headlines+'.csv',index_col=0, parse_dates=['timestamp'])\n",
    "    twitter8 = pd.read_csv(twitter_path+ticker4+'_2020-09-23_2020-'+latest_headlines+'.csv',index_col=0, parse_dates=['timestamp'])\n",
    "    twitter9 = pd.read_csv(twitter_path+ticker4+'_2020-09-23_2020-'+latest_headlines+'.csv',index_col=0, parse_dates=['timestamp'])\n",
    "    twitter10 = pd.read_csv(twitter_path+ticker4+'_2020-09-23_2020-'+latest_headlines+'.csv',index_col=0, parse_dates=['timestamp'])\n",
    "    twitter11 = pd.read_csv(twitter_path+ticker4+'_2020-09-23_2020-'+latest_headlines+'.csv',index_col=0, parse_dates=['timestamp'])\n",
    "    twitter12 = pd.read_csv(twitter_path+ticker4+'_2020-09-23_2020-'+latest_headlines+'.csv',index_col=0, parse_dates=['timestamp'])\n",
    "    twitter13 = pd.read_csv(twitter_path+ticker4+'_2020-09-23_2020-'+latest_headlines+'.csv',index_col=0, parse_dates=['timestamp'])\n",
    "\n",
    "\n",
    "    return stock_df1,headlines1,twitter1, stock_df2,headlines2,twitter2, stock_df3,headlines3,twitter3, stock_df4,headlines4,twitter4, stock_df5,headlines5,twitter5, stock_df6,headlines6,twitter6 , stock_df7,headlines7,twitter7, stock_df8,headlines8,twitter8, stock_df9,headlines9,twitter9, stock_df10,headlines10,twitter10, stock_df11,headlines11,twitter11, stock_df12,headlines12,twitter12, stock_df13,headlines13,twitter13"
   ]
  },
  {
   "source": [
    "# 1. Import Data:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_df1,headlines1,twitter1, stock_df2,headlines2,twitter2, stock_df3,headlines3,twitter3, stock_df4,headlines4,twitter4, stock_df5,headlines5,twitter5, stock_df6,headlines6,twitter6 , stock_df7,headlines7,twitter7, stock_df8,headlines8,twitter8, stock_df9,headlines9,twitter9, stock_df10,headlines10,twitter10, stock_df11,headlines11,twitter11, stock_df12,headlines12,twitter12, stock_df13,headlines13,twitter13 = import_data('TSLA','AMZN','AAPL','GOOG', 'FB', 'NFLX', 'CVX','GS','JNJ','NVDA','PFE','NKE','MSFT')"
   ]
  },
  {
   "source": [
    "# 2. Cleaning and Merging Data by Company:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsla_headlines_df, tsla_twitter_df, tsla_full_df = cleaning_df(stock_df1, headlines1, twitter1)\n",
    "amzn_headlines_df, amzn_twitter_df, amzn_full_df = cleaning_df(stock_df2, headlines2, twitter2)\n",
    "aapl_headlines_df, aapl_twitter_df, aapl_full_df = cleaning_df(stock_df3, headlines3, twitter3)\n",
    "goog_headlines_df, goog_twitter_df, goog_full_df = cleaning_df(stock_df4, headlines4, twitter4)\n",
    "\n",
    "fb_headlines_df, fb_twitter_df, fb_full_df = cleaning_df(stock_df5, headlines5, twitter5)\n",
    "nflx_headlines_df, nflx_twitter_df, nflx_full_df = cleaning_df(stock_df6, headlines6, twitter6)\n",
    "cvx_headlines_df, cvx_twitter_df, cvx_full_df = cleaning_df(stock_df7, headlines7, twitter7)\n",
    "gs_headlines_df, gs_twitter_df, gs_full_df = cleaning_df(stock_df8, headlines8, twitter8)\n",
    "\n",
    "jnj_headlines_df, jnj_twitter_df, jnj_full_df = cleaning_df(stock_df9, headlines9, twitter9)\n",
    "nvda_headlines_df, nvda_twitter_df, nvda_full_df = cleaning_df(stock_df10, headlines10, twitter10)\n",
    "pfe_headlines_df, pfe_twitter_df, pfe_full_df = cleaning_df(stock_df11, headlines11, twitter11)\n",
    "nke_headlines_df, nke_twitter_df, nke_full_df = cleaning_df(stock_df12, headlines12, twitter12)\n",
    "\n",
    "msft_headlines_df, msft_twitter_df, msft_full_df = cleaning_df(stock_df13, headlines13, twitter13)"
   ]
  },
  {
   "source": [
    "# 3. Merging All Companies:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_frames = [stock_df1, stock_df2, stock_df3, stock_df4, stock_df5, stock_df6, stock_df7, stock_df8, stock_df9, stock_df10, stock_df11, stock_df12, stock_df13]\n",
    "full_stocks = pd.concat(stock_frames)\n",
    "\n",
    "headline_frames = [tsla_headlines_df, amzn_headlines_df, aapl_headlines_df, goog_headlines_df,fb_headlines_df,nflx_headlines_df,cvx_headlines_df,gs_headlines_df,jnj_headlines_df,nvda_headlines_df,pfe_headlines_df,nke_headlines_df,msft_headlines_df]\n",
    "full_headlines = pd.concat(headline_frames)\n",
    "\n",
    "twitter_frames = [tsla_twitter_df,amzn_twitter_df,aapl_twitter_df,goog_twitter_df,fb_twitter_df,nflx_twitter_df,cvx_twitter_df,gs_twitter_df,jnj_twitter_df,nvda_twitter_df,pfe_twitter_df,nke_twitter_df,msft_twitter_df]\n",
    "full_twitter = pd.concat(twitter_frames)\n",
    "\n",
    "full_frames = [tsla_full_df,amzn_full_df,aapl_full_df,goog_full_df,fb_full_df,nflx_full_df,cvx_full_df,gs_full_df,jnj_full_df,nvda_full_df,pfe_full_df,nke_full_df,msft_full_df]\n",
    "full_final = pd.concat(full_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "               Datetime    Adj Close    Volume  \\\n",
       "0   2020-09-23 15:00:00   381.969910   5364827   \n",
       "1   2020-09-23 15:30:00   380.388489   6172522   \n",
       "2   2020-09-24 09:30:00   371.984985  18095098   \n",
       "3   2020-09-24 10:00:00   372.038391  10354746   \n",
       "4   2020-09-24 10:30:00   381.149994   7679126   \n",
       "..                  ...          ...       ...   \n",
       "135 2020-10-07 13:30:00  1459.349976     97814   \n",
       "136 2020-10-07 14:00:00  1464.050049     87885   \n",
       "137 2020-10-07 14:30:00  1461.979980     84749   \n",
       "138 2020-10-07 15:00:00  1460.869995    112529   \n",
       "139 2020-10-07 15:30:00  1460.290039    259542   \n",
       "\n",
       "     Percent Price Change Within Period  Scaled Volume       SMA(3)  \\\n",
       "0                              1.146567       1.188751   383.644958   \n",
       "1                             -0.382746       1.367722   381.524933   \n",
       "2                              1.424631       4.009554   380.002797   \n",
       "3                              0.012552       2.294429   378.114461   \n",
       "4                              2.456919       1.701559   374.803955   \n",
       "..                                  ...            ...          ...   \n",
       "135                            0.258315       1.016169  1451.601685   \n",
       "136                            0.298013       0.913019  1454.893433   \n",
       "137                           -0.169344       0.880440  1459.256673   \n",
       "138                           -0.071142       1.169040  1461.793335   \n",
       "139                           -0.082105       2.696327  1462.300008   \n",
       "\n",
       "             t+1  scaled_followers_count  compound_x  Compound SMA(3) Twitter  \\\n",
       "0     380.388489                0.000182    0.000000                 0.000000   \n",
       "1     371.984985                0.000122    0.000000                 0.000000   \n",
       "2     372.038391                0.000142    0.361234                 0.361234   \n",
       "3     381.149994                0.000142    0.361234                 0.361234   \n",
       "4     387.019989                0.000142    0.361234                 0.361234   \n",
       "..           ...                     ...         ...                      ...   \n",
       "135  1464.050049                0.022524    0.238175                 0.083535   \n",
       "136  1461.979980                0.000901    0.000000                 0.049177   \n",
       "137  1460.869995                0.003868    0.051623                 0.096599   \n",
       "138  1460.290039                0.000720    0.000000                 0.017208   \n",
       "139          NaN                0.000539    0.000000                 0.017208   \n",
       "\n",
       "     change in sentiment twitter  change in sentiment twitter (t-1)  \\\n",
       "0                       0.000000                           0.000000   \n",
       "1                       0.000000                           0.000000   \n",
       "2                       0.000000                           0.000000   \n",
       "3                       0.000000                           0.000000   \n",
       "4                       0.000000                           0.000000   \n",
       "..                           ...                                ...   \n",
       "135                     0.328818                          -0.193715   \n",
       "136                     0.000000                           0.328818   \n",
       "137                     0.051623                           0.000000   \n",
       "138                     0.000000                           0.051623   \n",
       "139                     0.000000                           0.000000   \n",
       "\n",
       "     compound_y  Compound SMA(3) Headlines  change in sentiment headlines  \\\n",
       "0        0.3021                   0.030450                        0.51285   \n",
       "1       -0.2202                  -0.042950                       -0.52230   \n",
       "2        0.0000                  -0.098667                        0.00000   \n",
       "3        0.0000                   0.000000                        0.00000   \n",
       "4        0.0000                   0.000000                        0.00000   \n",
       "..          ...                        ...                            ...   \n",
       "135      0.2787                   0.077567                        0.07775   \n",
       "136      0.2916                   0.257083                        0.01290   \n",
       "137      0.2916                   0.287300                        0.00000   \n",
       "138     -0.2263                   0.118967                       -0.51790   \n",
       "139      0.0000                   0.021767                        0.00000   \n",
       "\n",
       "     change in sentiment headlines (t-1)  \\\n",
       "0                               -0.21075   \n",
       "1                                0.51285   \n",
       "2                                0.00000   \n",
       "3                                0.00000   \n",
       "4                                0.00000   \n",
       "..                                   ...   \n",
       "135                              0.44790   \n",
       "136                              0.07775   \n",
       "137                              0.01290   \n",
       "138                              0.00000   \n",
       "139                             -0.51790   \n",
       "\n",
       "     Percent Price Change Within Period (t+1)  \n",
       "0                                   -0.382746  \n",
       "1                                    1.424631  \n",
       "2                                    0.012552  \n",
       "3                                    2.456919  \n",
       "4                                    1.564056  \n",
       "..                                        ...  \n",
       "135                                  0.298013  \n",
       "136                                 -0.169344  \n",
       "137                                 -0.071142  \n",
       "138                                 -0.082105  \n",
       "139                                       NaN  \n",
       "\n",
       "[1803 rows x 17 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Datetime</th>\n      <th>Adj Close</th>\n      <th>Volume</th>\n      <th>Percent Price Change Within Period</th>\n      <th>Scaled Volume</th>\n      <th>SMA(3)</th>\n      <th>t+1</th>\n      <th>scaled_followers_count</th>\n      <th>compound_x</th>\n      <th>Compound SMA(3) Twitter</th>\n      <th>change in sentiment twitter</th>\n      <th>change in sentiment twitter (t-1)</th>\n      <th>compound_y</th>\n      <th>Compound SMA(3) Headlines</th>\n      <th>change in sentiment headlines</th>\n      <th>change in sentiment headlines (t-1)</th>\n      <th>Percent Price Change Within Period (t+1)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2020-09-23 15:00:00</td>\n      <td>381.969910</td>\n      <td>5364827</td>\n      <td>1.146567</td>\n      <td>1.188751</td>\n      <td>383.644958</td>\n      <td>380.388489</td>\n      <td>0.000182</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.3021</td>\n      <td>0.030450</td>\n      <td>0.51285</td>\n      <td>-0.21075</td>\n      <td>-0.382746</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2020-09-23 15:30:00</td>\n      <td>380.388489</td>\n      <td>6172522</td>\n      <td>-0.382746</td>\n      <td>1.367722</td>\n      <td>381.524933</td>\n      <td>371.984985</td>\n      <td>0.000122</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>-0.2202</td>\n      <td>-0.042950</td>\n      <td>-0.52230</td>\n      <td>0.51285</td>\n      <td>1.424631</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2020-09-24 09:30:00</td>\n      <td>371.984985</td>\n      <td>18095098</td>\n      <td>1.424631</td>\n      <td>4.009554</td>\n      <td>380.002797</td>\n      <td>372.038391</td>\n      <td>0.000142</td>\n      <td>0.361234</td>\n      <td>0.361234</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0000</td>\n      <td>-0.098667</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.012552</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2020-09-24 10:00:00</td>\n      <td>372.038391</td>\n      <td>10354746</td>\n      <td>0.012552</td>\n      <td>2.294429</td>\n      <td>378.114461</td>\n      <td>381.149994</td>\n      <td>0.000142</td>\n      <td>0.361234</td>\n      <td>0.361234</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>2.456919</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2020-09-24 10:30:00</td>\n      <td>381.149994</td>\n      <td>7679126</td>\n      <td>2.456919</td>\n      <td>1.701559</td>\n      <td>374.803955</td>\n      <td>387.019989</td>\n      <td>0.000142</td>\n      <td>0.361234</td>\n      <td>0.361234</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>1.564056</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>135</th>\n      <td>2020-10-07 13:30:00</td>\n      <td>1459.349976</td>\n      <td>97814</td>\n      <td>0.258315</td>\n      <td>1.016169</td>\n      <td>1451.601685</td>\n      <td>1464.050049</td>\n      <td>0.022524</td>\n      <td>0.238175</td>\n      <td>0.083535</td>\n      <td>0.328818</td>\n      <td>-0.193715</td>\n      <td>0.2787</td>\n      <td>0.077567</td>\n      <td>0.07775</td>\n      <td>0.44790</td>\n      <td>0.298013</td>\n    </tr>\n    <tr>\n      <th>136</th>\n      <td>2020-10-07 14:00:00</td>\n      <td>1464.050049</td>\n      <td>87885</td>\n      <td>0.298013</td>\n      <td>0.913019</td>\n      <td>1454.893433</td>\n      <td>1461.979980</td>\n      <td>0.000901</td>\n      <td>0.000000</td>\n      <td>0.049177</td>\n      <td>0.000000</td>\n      <td>0.328818</td>\n      <td>0.2916</td>\n      <td>0.257083</td>\n      <td>0.01290</td>\n      <td>0.07775</td>\n      <td>-0.169344</td>\n    </tr>\n    <tr>\n      <th>137</th>\n      <td>2020-10-07 14:30:00</td>\n      <td>1461.979980</td>\n      <td>84749</td>\n      <td>-0.169344</td>\n      <td>0.880440</td>\n      <td>1459.256673</td>\n      <td>1460.869995</td>\n      <td>0.003868</td>\n      <td>0.051623</td>\n      <td>0.096599</td>\n      <td>0.051623</td>\n      <td>0.000000</td>\n      <td>0.2916</td>\n      <td>0.287300</td>\n      <td>0.00000</td>\n      <td>0.01290</td>\n      <td>-0.071142</td>\n    </tr>\n    <tr>\n      <th>138</th>\n      <td>2020-10-07 15:00:00</td>\n      <td>1460.869995</td>\n      <td>112529</td>\n      <td>-0.071142</td>\n      <td>1.169040</td>\n      <td>1461.793335</td>\n      <td>1460.290039</td>\n      <td>0.000720</td>\n      <td>0.000000</td>\n      <td>0.017208</td>\n      <td>0.000000</td>\n      <td>0.051623</td>\n      <td>-0.2263</td>\n      <td>0.118967</td>\n      <td>-0.51790</td>\n      <td>0.00000</td>\n      <td>-0.082105</td>\n    </tr>\n    <tr>\n      <th>139</th>\n      <td>2020-10-07 15:30:00</td>\n      <td>1460.290039</td>\n      <td>259542</td>\n      <td>-0.082105</td>\n      <td>2.696327</td>\n      <td>1462.300008</td>\n      <td>NaN</td>\n      <td>0.000539</td>\n      <td>0.000000</td>\n      <td>0.017208</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0000</td>\n      <td>0.021767</td>\n      <td>0.00000</td>\n      <td>-0.51790</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>1803 rows × 17 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "full_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model Saved!\nRMSE Score:  6.140317699900427e-05\n"
     ]
    }
   ],
   "source": [
    "full_final.dropna(inplace=True)\n",
    "multi_model_full(full_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}